import sys
import os
import requests
from bs4 import BeautifulSoup
from collections import Counter
from datetime import datetime

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ nlp ë””ë ‰í† ë¦¬ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../nlp")))

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from Nlp import Nlp  
    from MySQLDatabase import MySQLDatabase 
except ModuleNotFoundError as e:
    print(f"âŒ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    sys.exit(1)  # í”„ë¡œê·¸ë¨ ì¢…ë£Œ

def scrape_h2_text(url):
    """URLì—ì„œ <h2> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í¬ë˜í•‘"""
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"âŒ {url} ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
    soup = BeautifulSoup(response.text, 'html.parser')
    h2_tags = soup.find_all('h2')
    return [tag.get_text(strip=True) for tag in h2_tags]

if __name__ == "__main__":
    sections = {
        "politics": [
            "https://www.joongang.co.kr/politics",
            "https://www.joongang.co.kr/politics?page=2",
            "https://www.joongang.co.kr/politics?page=3"
        ],
        "sports": [
            "https://www.joongang.co.kr/sports",
            "https://www.joongang.co.kr/sports?page=2",
            "https://www.joongang.co.kr/sports?page=3"
        ],
        "economic": [
            "https://www.joongang.co.kr/money",
            "https://www.joongang.co.kr/money?page=2",
            "https://www.joongang.co.kr/money?page=3"
        ],
        "society": [
            "https://www.joongang.co.kr/society",
            "https://www.joongang.co.kr/society?page=2",
            "https://www.joongang.co.kr/society?page=3"
        ],
        "world": [
            "https://www.joongang.co.kr/world",
            "https://www.joongang.co.kr/world?page=2",
            "https://www.joongang.co.kr/world?page=3"
        ],

    }

    processor = Nlp()
    db = MySQLDatabase()

    for section, urls in sections.items():
        all_nouns = []
        for url in urls:
            h2_texts = scrape_h2_text(url)
            result = processor.KonlpyOkt(h2_texts)
            all_nouns.extend(result)

        # ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶”ì¶œ
        word_counts = Counter(all_nouns)
        top_10_words = [word for word, _ in word_counts.most_common(10)]

        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œë¥¼ DBì— ì €ì¥ (í˜„ì¬ datetime í¬í•¨)
        db.insert_top_keywords(section, top_10_words)

        print(f"\nğŸŸ¢ ìŠ¤í¬ë˜í•‘í•œ ì „ì²´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ({section.upper()}):")
        print(all_nouns)
        print(f"\nğŸ”µ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ TOP 10 ({section.upper()}):")
        for word in top_10_words:
            print(word)

    # DBì— ì €ì¥ëœ í‚¤ì›Œë“œì™€ ë‚ ì§œ ì¡°íšŒ
    fetched_data = db.fetch_keywords()
    print("\nğŸ“Œ DBì—ì„œ ê°€ì ¸ì˜¨ í‚¤ì›Œë“œ ë° ë‚ ì§œ:")
    for news_date, keyword in fetched_data:
        print(f"{news_date} - {keyword}")

    db.close()


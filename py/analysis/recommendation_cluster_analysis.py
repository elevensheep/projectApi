#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ë¶„ì„
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
from gensim.models import Word2Vec
import platform
from collections import Counter

def load_word2vec_model():
    """Word2Vec ëª¨ë¸ ë¡œë“œ"""
    try:
        model_path = "word2vec.model"
        if os.path.exists(model_path):
            model = Word2Vec.load(model_path)
            print(f"âœ… Word2Vec ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ğŸ“Š ëª¨ë¸ ì–´íœ˜ í¬ê¸°: {len(model.wv.key_to_index)}")
            return model
        else:
            print("âŒ word2vec.model íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def get_word_vectors(model, max_words=1000):
    """ë‹¨ì–´ ë²¡í„° ì¶”ì¶œ"""
    word_list = list(model.wv.key_to_index)[:max_words]
    valid_words = [word for word in word_list if word in model.wv]
    word_vectors = np.array([model.wv[word] for word in valid_words])
    
    print(f"ğŸ“Š {len(valid_words)}ê°œì˜ ë‹¨ì–´ ë²¡í„°ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    return valid_words, word_vectors

def analyze_recommendation_clusters(words, word_vectors, model, k_values=[5, 8, 10, 12, 15]):
    """ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ë¶„ì„"""
    print("ğŸ” ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    print("=" * 80)
    
    # ì°¨ì› ì¶•ì†Œ
    pca = PCA(n_components=min(50, len(word_vectors[0])))
    word_vectors_pca = pca.fit_transform(word_vectors)
    
    tsne = TSNE(
        n_components=2,
        perplexity=min(30, len(word_vectors) - 1),
        learning_rate=200,
        n_iter=1000,
        random_state=42,
        init='pca'
    )
    reduced_vectors = tsne.fit_transform(word_vectors_pca)
    
    # ê° K ê°’ì— ëŒ€í•œ ë¶„ì„
    results = {}
    
    for k in k_values:
        print(f"\nğŸ”¸ K={k} í´ëŸ¬ìŠ¤í„° ë¶„ì„:")
        print("-" * 50)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(reduced_vectors)
        
        # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
        silhouette_avg = silhouette_score(reduced_vectors, labels)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë‹¨ì–´ ë¶„ì„
        cluster_groups = {}
        for i, (word, label) in enumerate(zip(words, labels)):
            if label not in cluster_groups:
                cluster_groups[label] = []
            cluster_groups[label].append(word)
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„ì„
        cluster_sizes = [len(cluster_groups[i]) for i in range(k)]
        avg_cluster_size = np.mean(cluster_sizes)
        min_cluster_size = min(cluster_sizes)
        max_cluster_size = max(cluster_sizes)
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ìœ ì‚¬ë„ ë¶„ì„
        intra_cluster_similarities = []
        for cluster_id in range(k):
            cluster_words = cluster_groups[cluster_id]
            if len(cluster_words) > 1:
                # í´ëŸ¬ìŠ¤í„° ë‚´ ë‹¨ì–´ë“¤ì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
                similarities = []
                for i, word1 in enumerate(cluster_words):
                    for j, word2 in enumerate(cluster_words[i+1:], i+1):
                        if word1 in model.wv and word2 in model.wv:
                            sim = model.wv.similarity(word1, word2)
                            similarities.append(sim)
                if similarities:
                    intra_cluster_similarities.append(np.mean(similarities))
        
        avg_intra_similarity = np.mean(intra_cluster_similarities) if intra_cluster_similarities else 0
        
        results[k] = {
            'silhouette': silhouette_avg,
            'cluster_sizes': cluster_sizes,
            'avg_cluster_size': avg_cluster_size,
            'min_cluster_size': min_cluster_size,
            'max_cluster_size': max_cluster_size,
            'avg_intra_similarity': avg_intra_similarity,
            'cluster_groups': cluster_groups
        }
        
        print(f"   ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.3f}")
        print(f"   í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {avg_cluster_size:.1f}")
        print(f"   ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {min_cluster_size}")
        print(f"   ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {max_cluster_size}")
        print(f"   í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ìœ ì‚¬ë„: {avg_intra_similarity:.3f}")
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ë‹¨ì–´ ì¶œë ¥
        print(f"   í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ë‹¨ì–´:")
        for cluster_id in range(min(3, k)):  # ì²˜ìŒ 3ê°œ í´ëŸ¬ìŠ¤í„°ë§Œ ì¶œë ¥
            cluster_words = cluster_groups[cluster_id]
            representative_words = cluster_words[:8]  # ì²˜ìŒ 8ê°œ ë‹¨ì–´
            print(f"     í´ëŸ¬ìŠ¤í„° {cluster_id}: {', '.join(representative_words)}")
    
    return results, reduced_vectors

def plot_recommendation_clusters(words, reduced_vectors, k_values=[5, 8, 10, 12, 15]):
    """ì¶”ì²œ ì‹œìŠ¤í…œìš© í´ëŸ¬ìŠ¤í„° ì‹œê°í™”"""
    # ê·¸ë˜í”„ ì„¤ì •
    if platform.system() == "Windows":
        plt.rcParams['font.family'] = 'Malgun Gothic'
    else:
        plt.rcParams['font.family'] = 'Nanum Gothic'
    
    fig, axes = plt.subplots(2, 3, figsize=(24, 16))
    axes = axes.ravel()
    
    for i, k in enumerate(k_values):
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(reduced_vectors)
        
        # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
        silhouette_avg = silhouette_score(reduced_vectors, labels)
        
        # ì‹œê°í™”
        colors = plt.cm.tab20(np.linspace(0, 1, k))
        for cluster_id in range(k):
            cluster_mask = labels == cluster_id
            axes[i].scatter(reduced_vectors[cluster_mask, 0], 
                          reduced_vectors[cluster_mask, 1], 
                          c=[colors[cluster_id]], 
                          alpha=0.7, s=30, 
                          label=f'C{cluster_id}')
        
        axes[i].set_title(f'K={k} í´ëŸ¬ìŠ¤í„°\nì‹¤ë£¨ì—£: {silhouette_avg:.3f}', 
                         fontsize=12, fontweight='bold')
        axes[i].set_xlabel('t-SNE-1', fontsize=10)
        axes[i].set_ylabel('t-SNE-2', fontsize=10)
        axes[i].grid(True, alpha=0.3)
        if k <= 10:  # í´ëŸ¬ìŠ¤í„°ê°€ ë§ìœ¼ë©´ ë²”ë¡€ ìƒëµ
            axes[i].legend(fontsize=8)
    
    # ë§ˆì§€ë§‰ subplot ì œê±°
    axes[-1].remove()
    
    plt.tight_layout()
    plt.savefig("recommendation_clusters.png", dpi=300, bbox_inches='tight')
    print("ğŸ“Š ì¶”ì²œ ì‹œìŠ¤í…œìš© í´ëŸ¬ìŠ¤í„° ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: recommendation_clusters.png")
    plt.show()

def recommend_optimal_k_for_recommendation(results):
    """ì¶”ì²œ ì‹œìŠ¤í…œì— ìµœì ì¸ K ê°’ ì¶”ì²œ"""
    print("\n" + "=" * 80)
    print("ğŸ¯ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ ìµœì  K ê°’ ë¶„ì„")
    print("=" * 80)
    
    # ì¶”ì²œ ì‹œìŠ¤í…œì— ì¤‘ìš”í•œ ìš”ì†Œë“¤
    print("\nğŸ“Š ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ê¸°ì¤€:")
    print("1. ì‹¤ë£¨ì—£ ì ìˆ˜ (í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ)")
    print("2. í´ëŸ¬ìŠ¤í„° í¬ê¸° ê· í˜• (ë„ˆë¬´ ì‘ê±°ë‚˜ í¬ì§€ ì•ŠìŒ)")
    print("3. í´ëŸ¬ìŠ¤í„° ë‚´ ìœ ì‚¬ë„ (ê´€ë ¨ ë‹¨ì–´ë“¤ì´ í•¨ê»˜ ê·¸ë£¹í™”)")
    print("4. í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ì ë‹¹í•œ ì„¸ë¶„í™”)")
    
    print("\nğŸ“ˆ ê° K ê°’ë³„ í‰ê°€:")
    print("-" * 60)
    
    scores = {}
    for k, result in results.items():
        # ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„ë¡œ ì •ê·œí™”)
        silhouette_score = result['silhouette']
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ê· í˜• ì ìˆ˜ (í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
        size_std = np.std(result['cluster_sizes'])
        size_balance_score = 1 / (1 + size_std / result['avg_cluster_size'])
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ìœ ì‚¬ë„ ì ìˆ˜
        similarity_score = result['avg_intra_similarity']
        
        # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ì ìˆ˜ (5-15 ë²”ìœ„ì—ì„œ ì ë‹¹)
        if 5 <= k <= 15:
            count_score = 1 - abs(k - 10) / 10
        else:
            count_score = 0.5
        
        # ì¢…í•© ì ìˆ˜ (ê°€ì¤‘í‰ê· )
        total_score = (silhouette_score * 0.3 + 
                      size_balance_score * 0.2 + 
                      similarity_score * 0.3 + 
                      count_score * 0.2)
        
        scores[k] = {
            'total': total_score,
            'silhouette': silhouette_score,
            'size_balance': size_balance_score,
            'similarity': similarity_score,
            'count': count_score
        }
        
        print(f"K={k:2d}: ì´ì ={total_score:.3f} (ì‹¤ë£¨ì—£={silhouette_score:.3f}, "
              f"í¬ê¸°ê· í˜•={size_balance_score:.3f}, ìœ ì‚¬ë„={similarity_score:.3f}, "
              f"ê°œìˆ˜={count_score:.3f})")
    
    # ìµœì  K ê°’ ì°¾ê¸°
    best_k = max(scores.keys(), key=lambda k: scores[k]['total'])
    best_score = scores[best_k]['total']
    
    print(f"\nğŸ† ì¶”ì²œ ì‹œìŠ¤í…œì— ìµœì ì¸ K ê°’: {best_k}")
    print(f"   ì¢…í•© ì ìˆ˜: {best_score:.3f}")
    
    print(f"\nğŸ’¡ K={best_k}ë¥¼ ì„ íƒí•˜ëŠ” ì´ìœ :")
    print(f"   - ì‹¤ë£¨ì—£ ì ìˆ˜: {scores[best_k]['silhouette']:.3f} (í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ)")
    print(f"   - í¬ê¸° ê· í˜•: {scores[best_k]['size_balance']:.3f} (ê· ë“±í•œ ë¶„í¬)")
    print(f"   - ë‚´ë¶€ ìœ ì‚¬ë„: {scores[best_k]['similarity']:.3f} (ê´€ë ¨ ë‹¨ì–´ ê·¸ë£¹í™”)")
    print(f"   - í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {scores[best_k]['count']:.3f} (ì ë‹¹í•œ ì„¸ë¶„í™”)")
    
    return best_k, scores

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ”¬ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ë¶„ì„ í”„ë¡œê·¸ë¨")
    print("=" * 80)
    
    # Word2Vec ëª¨ë¸ ë¡œë“œ
    model = load_word2vec_model()
    if model is None:
        return
    
    # ë‹¨ì–´ ë²¡í„° ì¶”ì¶œ
    words, word_vectors = get_word_vectors(model, max_words=1000)
    
    if len(word_vectors) < 10:
        print("âŒ ì¶©ë¶„í•œ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ë¶„ì„
    results, reduced_vectors = analyze_recommendation_clusters(words, word_vectors, model)
    
    # í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
    print("\nğŸ“Š í´ëŸ¬ìŠ¤í„° ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    plot_recommendation_clusters(words, reduced_vectors)
    
    # ìµœì  K ê°’ ì¶”ì²œ
    best_k, scores = recommend_optimal_k_for_recommendation(results)
    
    print("\n" + "=" * 80)
    print("ğŸ¯ ê²°ë¡ :")
    print("=" * 80)
    print(f"âœ… ì¶”ì²œ ì‹œìŠ¤í…œì—ëŠ” K={best_k}ê°€ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤!")
    print(f"   - ê´€ë ¨ì„±ì´ ë†’ì€ ë‹¨ì–´ë“¤ì´ í•¨ê»˜ ê·¸ë£¹í™”ë©ë‹ˆë‹¤")
    print(f"   - ì ë‹¹í•œ ì„¸ë¶„í™”ë¡œ ì •í™•í•œ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    print(f"   - í´ëŸ¬ìŠ¤í„° í¬ê¸°ê°€ ê· í˜•ì¡í˜€ ìˆìŠµë‹ˆë‹¤")
    print("\nğŸ’¡ K=2ëŠ” ì¶”ì²œ ì‹œìŠ¤í…œì— ë¶€ì í•©í•©ë‹ˆë‹¤:")
    print("   - ë„ˆë¬´ ë‹¨ìˆœí•œ ë¶„ë¥˜ë¡œ ì„¸ë°€í•œ ì¶”ì²œ ë¶ˆê°€")
    print("   - ê´€ë ¨ ë‹¨ì–´ë“¤ì„ ì„¸ë°€í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ì—†ìŒ")
    print("   - ì¶”ì²œ ì •í™•ë„ê°€ ë–¨ì–´ì§")
    print("=" * 80)

if __name__ == "__main__":
    main()

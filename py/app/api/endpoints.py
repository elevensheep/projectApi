#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
FastAPI ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
- ì¶”ì²œ ì‹œìŠ¤í…œ API
- ëª¨ë¸ ì‹œê°í™” API
- ìºì‹± ì‹œìŠ¤í…œ
"""

import logging
import time
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, Query, Path, Depends, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import mysql.connector

# ìƒëŒ€ ê²½ë¡œë¡œ import ìˆ˜ì •
from services.database import MySQLDatabase
from services.nlp import Nlp

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

router = APIRouter()

# Pydantic ëª¨ë¸
class BookResponse(BaseModel):
    """ì±… ì •ë³´ ì‘ë‹µ ëª¨ë¸"""
    books_isbn: str = Field(..., description="ì±… ISBN")
    news_category: str = Field(..., description="ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬")
    books_img: Optional[str] = Field(None, description="ì±… ì´ë¯¸ì§€ URL")
    books_description: Optional[str] = Field(None, description="ì±… ì„¤ëª…")
    books_title: str = Field(..., description="ì±… ì œëª©")
    books_publisher: Optional[str] = Field(None, description="ì¶œíŒì‚¬")
    similarity_score: Optional[float] = Field(None, description="ìœ ì‚¬ë„ ì ìˆ˜")

class RecommendationResponse(BaseModel):
    """ì¶”ì²œ ì‘ë‹µ ëª¨ë¸"""
    total: int = Field(..., description="ì „ì²´ ì±… ìˆ˜")
    page: int = Field(..., description="í˜„ì¬ í˜ì´ì§€")
    limit: int = Field(..., description="í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜")
    total_pages: int = Field(..., description="ì „ì²´ í˜ì´ì§€ ìˆ˜")
    books: List[BookResponse] = Field(..., description="ì±… ëª©ë¡")
    cache_hit: bool = Field(False, description="ìºì‹œ íˆíŠ¸ ì—¬ë¶€")

class ErrorResponse(BaseModel):
    """ì—ëŸ¬ ì‘ë‹µ ëª¨ë¸"""
    error: bool = Field(True, description="ì—ëŸ¬ ì—¬ë¶€")
    message: str = Field(..., description="ì—ëŸ¬ ë©”ì‹œì§€")
    details: Optional[Dict[str, Any]] = Field(None, description="ìƒì„¸ ì •ë³´")

# ìºì‹œ ì‹œìŠ¤í…œ
recommendation_cache: Dict[str, Dict[str, Any]] = {}
cache_timestamps: Dict[str, float] = {}
CACHE_TTL = 3600  # 1ì‹œê°„

def get_cache_key(category: str, date: Optional[str], page: int, limit: int) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    return f"{category}:{date or 'all'}:{page}:{limit}"

def is_cache_valid(cache_key: str) -> bool:
    """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
    if cache_key not in cache_timestamps:
        return False
    return time.time() - cache_timestamps[cache_key] < CACHE_TTL

def get_cached_data(cache_key: str) -> Optional[Dict[str, Any]]:
    """ìºì‹œëœ ë°ì´í„° ì¡°íšŒ"""
    if is_cache_valid(cache_key):
        logger.info(f"ğŸ“¦ ìºì‹œ íˆíŠ¸: {cache_key}")
        return recommendation_cache.get(cache_key)
    return None

def set_cached_data(cache_key: str, data: Dict[str, Any]):
    """ë°ì´í„° ìºì‹±"""
    recommendation_cache[cache_key] = data
    cache_timestamps[cache_key] = time.time()
    logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {cache_key}")

# ë°ì´í„°ë² ì´ìŠ¤ ì˜ì¡´ì„±
def get_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜ì¡´ì„±"""
    db = MySQLDatabase()
    try:
        yield db
    finally:
        db.close()

@router.get(
    "/recommend/{category}",
    response_model=RecommendationResponse,
    summary="ì¹´í…Œê³ ë¦¬ë³„ ë„ì„œ ì¶”ì²œ",
    description="íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ê¸°ë°˜ ë„ì„œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.",
    responses={
        200: {"description": "ì„±ê³µì ìœ¼ë¡œ ì¶”ì²œ ë„ì„œë¥¼ ë°˜í™˜"},
        400: {"model": ErrorResponse, "description": "ì˜ëª»ëœ ìš”ì²­"},
        404: {"model": ErrorResponse, "description": "ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"model": ErrorResponse, "description": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜"}
    }
)
async def get_recommendations(
    category: str = Path(..., description="ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ (politics, sports, economic, society, world)"),
    date: Optional[str] = Query(None, alias="news_date", description="ë‰´ìŠ¤ ë‚ ì§œ (YYYY-MM-DD)"),
    page: int = Query(1, gt=0, le=1000, description="í˜ì´ì§€ ë²ˆí˜¸"),
    limit: int = Query(10, gt=0, le=100, description="í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜"),
    db: MySQLDatabase = Depends(get_database)
):
    """
    ì¹´í…Œê³ ë¦¬ë³„ ë„ì„œ ì¶”ì²œ API
    
    - **category**: ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ (politics, sports, economic, society, world)
    - **date**: ë‰´ìŠ¤ ë‚ ì§œ (ì„ íƒì‚¬í•­, YYYY-MM-DD í˜•ì‹)
    - **page**: í˜ì´ì§€ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)
    - **limit**: í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜ (ê¸°ë³¸ê°’: 10, ìµœëŒ€: 100)
    """
    
    start_time = time.time()
    
    try:
        # ì…ë ¥ ê²€ì¦ ë° ì˜¤íƒ€ ìˆ˜ì •
        valid_categories = ["politics", "sports", "economic", "society", "world"]
        
        # ì˜¤íƒ€ ìˆ˜ì • (ì¼ë°˜ì ì¸ ì˜¤íƒ€ë“¤)
        category_fixes = {
            "ecomonic": "economic",
            "economy": "economic",
            "politic": "politics",
            "sport": "sports",
            "social": "society",
            "international": "world"
        }
        
        # ì˜¤íƒ€ ìˆ˜ì • ì ìš©
        if category in category_fixes:
            original_category = category
            category = category_fixes[category]
            logger.info(f"ğŸ”§ ì¹´í…Œê³ ë¦¬ ì˜¤íƒ€ ìˆ˜ì •: '{original_category}' â†’ '{category}'")
        
        if category not in valid_categories:
            raise HTTPException(
                status_code=400,
                detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤. í—ˆìš©ëœ ê°’: {', '.join(valid_categories)}"
            )
        
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
        if date:
            try:
                time.strptime(date, "%Y-%m-%d")
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail="ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”."
                )
        
        # ìºì‹œ í™•ì¸
        cache_key = get_cache_key(category, date, page, limit)
        cached_data = get_cached_data(cache_key)
        
        if cached_data:
            cached_data["cache_hit"] = True
            return RecommendationResponse(**cached_data)
        
        # ì˜¤í”„ì…‹ ê³„ì‚°
        offset = (page - 1) * limit
        
        logger.info(f"ğŸ“¥ ì¶”ì²œ ìš”ì²­: category={category}, date={date}, page={page}, limit={limit}")
        
        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        count_query = """
            SELECT COUNT(DISTINCT r.books_isbn)
            FROM tb_recommend r
            JOIN tb_news_keyword n ON r.news_id = n.news_id
            JOIN tb_books b ON r.books_isbn = b.books_isbn
            WHERE n.news_category = %s
        """
        count_params = [category]
        
        if date:
            count_query += " AND DATE(n.news_date) = %s"
            count_params.append(date)
        
        try:
            total_result = db.fetch_query(count_query, count_params)
            total = total_result[0][0] if total_result else 0
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail="ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        if total == 0:
            raise HTTPException(
                status_code=404,
                detail=f"'{category}' ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ì¶”ì²œ ë„ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # ë°ì´í„° ì¡°íšŒ
        data_query = """
            SELECT DISTINCT
                b.books_isbn, n.news_category, b.books_img,
                b.books_description, b.books_title, b.books_publisher,
                n.news_date, r.similarity_score
            FROM tb_recommend r
            JOIN tb_news_keyword n ON r.news_id = n.news_id
            JOIN tb_books b ON r.books_isbn = b.books_isbn
            WHERE n.news_category = %s
        """
        data_params = [category]
        
        if date:
            data_query += " AND DATE(n.news_date) = %s"
            data_params.append(date)
        
        data_query += " ORDER BY r.similarity_score DESC, n.news_date DESC LIMIT %s OFFSET %s"
        data_params.extend([limit, offset])
        
        try:
            rows = db.fetch_query(data_query, data_params)
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail="ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        books = [
            BookResponse(
                books_isbn=row[0],
                news_category=row[1],
                books_img=row[2],
                books_description=row[3],
                books_title=row[4],
                books_publisher=row[5],
                similarity_score=float(row[7]) if row[7] else None
            )
            for row in rows
        ]
        
        total_pages = (total + limit - 1) // limit
        
        response_data = {
            "total": total,
            "page": page,
            "limit": limit,
            "total_pages": total_pages,
            "books": books,
            "cache_hit": False
        }
        
        # ìºì‹œ ì €ì¥
        set_cached_data(cache_key, response_data)
        
        process_time = time.time() - start_time
        logger.info(f"ğŸ“¤ ì¶”ì²œ ì‘ë‹µ: {len(books)}ê¶Œ, {process_time:.3f}ì´ˆ")
        
        return RecommendationResponse(**response_data)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì¶”ì²œ API ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get(
    "/visualize",
    summary="ëª¨ë¸ ì‹œê°í™”",
    description="NLP ëª¨ë¸ì˜ ì‹œê°í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.",
    responses={
        200: {"description": "ì‹œê°í™” ì™„ë£Œ"},
        500: {"model": ErrorResponse, "description": "ì‹œê°í™” ì‹¤íŒ¨"}
    }
)
async def visualize_model():
    """NLP ëª¨ë¸ ì‹œê°í™”"""
    try:
        logger.info("ğŸ“Š NLP ëª¨ë¸ ì‹œê°í™” ì‹œì‘")
        nlp = Nlp()
        nlp.LoadModel()
        nlp.VisualizeModel()
        logger.info("âœ… NLP ëª¨ë¸ ì‹œê°í™” ì™„ë£Œ")
        return {"message": "âœ… ëª¨ë¸ ì‹œê°í™” ì™„ë£Œ (ë¡œì»¬ì—ì„œ ì‹¤í–‰ë¨)"}
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ëª¨ë¸ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get(
    "/categories",
    summary="ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬",
    description="ì‚¬ìš© ê°€ëŠ¥í•œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
)
async def get_categories():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡"""
    categories = [
        {"id": "politics", "name": "ì •ì¹˜", "description": "ì •ì¹˜ ê´€ë ¨ ë‰´ìŠ¤"},
        {"id": "sports", "name": "ìŠ¤í¬ì¸ ", "description": "ìŠ¤í¬ì¸  ê´€ë ¨ ë‰´ìŠ¤"},
        {"id": "economic", "name": "ê²½ì œ", "description": "ê²½ì œ ê´€ë ¨ ë‰´ìŠ¤"},
        {"id": "society", "name": "ì‚¬íšŒ", "description": "ì‚¬íšŒ ê´€ë ¨ ë‰´ìŠ¤"},
        {"id": "world", "name": "êµ­ì œ", "description": "êµ­ì œ ê´€ë ¨ ë‰´ìŠ¤"}
    ]
    return {"categories": categories}

@router.get(
    "/cache/clear",
    summary="ìºì‹œ ì´ˆê¸°í™”",
    description="ì¶”ì²œ ì‹œìŠ¤í…œ ìºì‹œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
)
async def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    try:
        global recommendation_cache, cache_timestamps
        recommendation_cache.clear()
        cache_timestamps.clear()
        logger.info("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
        return {"message": "ìºì‹œê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ìºì‹œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get(
    "/cache/status",
    summary="ìºì‹œ ìƒíƒœ",
    description="í˜„ì¬ ìºì‹œ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
)
async def get_cache_status():
    """ìºì‹œ ìƒíƒœ í™•ì¸"""
    try:
        cache_size = len(recommendation_cache)
        cache_keys = list(recommendation_cache.keys())
        return {
            "cache_size": cache_size,
            "cache_keys": cache_keys,
            "cache_ttl": CACHE_TTL
        }
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ìºì‹œ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

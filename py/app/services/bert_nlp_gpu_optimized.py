#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GPU ìµœì í™”ëœ BERT NLP ì„œë¹„ìŠ¤
- CUDA ê°€ì†
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
"""

import os
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import platform
import re
from typing import List, Dict, Tuple, Optional
import logging
import time
import pickle
import hashlib
from concurrent.futures import ThreadPoolExecutor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GPUBertNLP:
    """
    GPU ìµœì í™”ëœ KoBERT ê¸°ë°˜ NLP ì„œë¹„ìŠ¤
    """
    
    def __init__(self, model_name: str = "skt/kobert-base-v1", cache_dir: str = "cache"):
        """
        GPU ìµœì í™”ëœ BERT NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  BERT ëª¨ë¸ëª…
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
        """
        self.model_name = model_name
        self.cache_dir = cache_dir
        
        # GPU ì„¤ì •
        self.device = self._setup_gpu()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.tokenizer = None
        self.model = None
        self.sentence_transformer = None
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        
        # ì„ë² ë”© ìºì‹œ
        self.embedding_cache = {}
        
        logger.info(f"ğŸš€ GPU ìµœì í™”ëœ BERT NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (Device: {self.device})")
        self._load_models()
        self._load_cache()
    
    def _setup_gpu(self) -> torch.device:
        """GPU ì„¤ì • ë° ìµœì í™”"""
        if torch.cuda.is_available():
            # GPU ì •ë³´ ì¶œë ¥
            gpu_count = torch.cuda.device_count()
            current_gpu = torch.cuda.current_device()
            gpu_name = torch.cuda.get_device_name(current_gpu)
            gpu_memory = torch.cuda.get_device_properties(current_gpu).total_memory / 1024**3
            
            logger.info(f"ğŸ® GPU ê°ì§€ë¨:")
            logger.info(f"   - GPU ê°œìˆ˜: {gpu_count}")
            logger.info(f"   - í˜„ì¬ GPU: {current_gpu}")
            logger.info(f"   - GPU ì´ë¦„: {gpu_name}")
            logger.info(f"   - GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
            torch.cuda.empty_cache()
            
            device = torch.device('cuda')
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            allocated = torch.cuda.memory_allocated(device) / 1024**2
            cached = torch.cuda.memory_reserved(device) / 1024**2
            logger.info(f"   - í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.1f}MB")
            logger.info(f"   - ìºì‹œëœ ë©”ëª¨ë¦¬: {cached:.1f}MB")
            
        else:
            logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            device = torch.device('cpu')
        
        return device
    
    def _load_models(self):
        """GPU ìµœì í™”ëœ BERT ëª¨ë¸ë“¤ ë¡œë“œ"""
        try:
            # KoBERT í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
            logger.info(f"KoBERT ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            
            # GPUë¡œ ëª¨ë¸ ì´ë™
            self.model.to(self.device)
            self.model.eval()
            
            # GPU ìµœì í™”
            if self.device.type == 'cuda':
                # ëª¨ë¸ ì»´íŒŒì¼ (PyTorch 2.0+)
                if hasattr(torch, 'compile'):
                    self.model = torch.compile(self.model, mode='reduce-overhead')
                
                # Mixed Precision ì„¤ì •
                self.use_amp = True
                self.scaler = torch.cuda.amp.GradScaler()
                
                logger.info("âœ… GPU ìµœì í™” ì ìš© ì™„ë£Œ")
            else:
                self.use_amp = False
                self.scaler = None
            
            # Sentence Transformer ë¡œë“œ (GPU ì§€ì›)
            logger.info("Sentence Transformer ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.sentence_transformer = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            if self.device.type == 'cuda':
                self.sentence_transformer.to(self.device)
            
            logger.info("âœ… ëª¨ë“  GPU ìµœì í™”ëœ BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_bert_embedding_gpu(self, text: str) -> np.ndarray:
        """
        GPU ìµœì í™”ëœ BERT ì„ë² ë”© ìƒì„±
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë¬¸ë§¥ ê¸°ë°˜ ì„ë² ë”© ë²¡í„°
        """
        if not text or not isinstance(text, str):
            return np.zeros(768)
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._get_cache_key(text)
        
        # ìºì‹œì—ì„œ í™•ì¸
        if cache_key in self.embedding_cache:
            return self.embedding_cache[cache_key]
        
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = self._preprocess_text(text)
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=256,
                padding=True
            )
            
            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # GPU ìµœì í™”ëœ ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                if self.use_amp:
                    # Mixed Precision ì‚¬ìš©
                    with torch.cuda.amp.autocast():
                        outputs = self.model(**inputs)
                        embedding = outputs.last_hidden_state[:, 0, :]
                else:
                    outputs = self.model(**inputs)
                    embedding = outputs.last_hidden_state[:, 0, :]
                
                # CPUë¡œ ì´ë™í•˜ì—¬ numpy ë³€í™˜
                embedding = embedding.cpu().numpy()
            
            result = embedding[0]
            
            # ìºì‹œì— ì €ì¥
            self.embedding_cache[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"GPU BERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros(768)
    
    def get_embeddings_batch_gpu(self, texts: List[str], batch_size: int = 64) -> List[np.ndarray]:
        """
        GPU ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
        
        Args:
            texts: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
            
        Returns:
            ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
        """
        embeddings = []
        
        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / 1024**3
            if gpu_memory < 4:  # 4GB ë¯¸ë§Œ
                batch_size = min(batch_size, 16)
            elif gpu_memory < 8:  # 8GB ë¯¸ë§Œ
                batch_size = min(batch_size, 32)
            else:  # 8GB ì´ìƒ
                batch_size = min(batch_size, 64)
        
        logger.info(f"ğŸ“¦ GPU ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = []
            
            # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
            batch_inputs = self.tokenizer(
                batch_texts,
                return_tensors="pt",
                truncation=True,
                max_length=256,
                padding=True
            )
            
            # GPUë¡œ ì´ë™
            batch_inputs = {k: v.to(self.device) for k, v in batch_inputs.items()}
            
            # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(**batch_inputs)
                        batch_embedding = outputs.last_hidden_state[:, 0, :]
                else:
                    outputs = self.model(**batch_inputs)
                    batch_embedding = outputs.last_hidden_state[:, 0, :]
                
                # CPUë¡œ ì´ë™í•˜ì—¬ numpy ë³€í™˜
                batch_embedding = batch_embedding.cpu().numpy()
            
            batch_embeddings.extend(batch_embedding)
            embeddings.extend(batch_embeddings)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        return embeddings
    
    def calculate_similarities_batch_gpu(self, query_text: str, candidate_texts: List[str]) -> List[float]:
        """
        GPU ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            query_text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            candidate_texts: í›„ë³´ í…ìŠ¤íŠ¸ë“¤
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.get_bert_embedding_gpu(query_text)
        
        # í›„ë³´ í…ìŠ¤íŠ¸ë“¤ ì„ë² ë”© ìƒì„± (GPU ë°°ì¹˜ ì²˜ë¦¬)
        candidate_embeddings = self.get_embeddings_batch_gpu(candidate_texts)
        
        # GPUì—ì„œ ìœ ì‚¬ë„ ê³„ì‚° (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš°)
        if len(candidate_embeddings) > 1000 and self.device.type == 'cuda':
            return self._calculate_similarities_gpu(query_embedding, candidate_embeddings)
        else:
            # CPUì—ì„œ ê³„ì‚° (ì†Œìš©ëŸ‰ ë°ì´í„°)
            similarities = []
            for candidate_embedding in candidate_embeddings:
                similarity = cosine_similarity([query_embedding], [candidate_embedding])[0][0]
                similarities.append(float(similarity))
            return similarities
    
    def _calculate_similarities_gpu(self, query_embedding: np.ndarray, candidate_embeddings: List[np.ndarray]) -> List[float]:
        """GPUì—ì„œ ëŒ€ìš©ëŸ‰ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # numpy ë°°ì—´ì„ GPU í…ì„œë¡œ ë³€í™˜
            query_tensor = torch.tensor(query_embedding, device=self.device, dtype=torch.float32)
            candidate_tensor = torch.tensor(candidate_embeddings, device=self.device, dtype=torch.float32)
            
            # ì •ê·œí™”
            query_norm = torch.norm(query_tensor)
            candidate_norms = torch.norm(candidate_tensor, dim=1, keepdim=True)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = torch.mm(candidate_tensor, query_tensor.unsqueeze(1)).squeeze(1)
            similarities = similarities / (candidate_norms.squeeze(1) * query_norm)
            
            # CPUë¡œ ì´ë™í•˜ì—¬ numpy ë³€í™˜
            similarities = similarities.cpu().numpy()
            
            return similarities.tolist()
            
        except Exception as e:
            logger.error(f"GPU ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # CPUë¡œ í´ë°±
            similarities = []
            for candidate_embedding in candidate_embeddings:
                similarity = cosine_similarity([query_embedding], [candidate_embedding])[0][0]
                similarities.append(float(similarity))
            return similarities
    
    def find_similar_texts_gpu(self, query_text: str, candidate_texts: List[str], 
                              top_k: int = 5, threshold: float = 0.3) -> List[Tuple[int, float]]:
        """
        GPU ìµœì í™”ëœ ìœ ì‚¬ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        
        Args:
            query_text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            candidate_texts: í›„ë³´ í…ìŠ¤íŠ¸ë“¤
            top_k: ë°˜í™˜í•  ìƒìœ„ ê°œìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            (ì¸ë±ìŠ¤, ìœ ì‚¬ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # GPU ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.calculate_similarities_batch_gpu(query_text, candidate_texts)
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹ ë¥¸ ì •ë ¬
            similarities_array = np.array(similarities)
            indices = np.argsort(similarities_array)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            
            results = []
            for idx in indices:
                if similarities_array[idx] >= threshold and len(results) < top_k:
                    results.append((idx, similarities_array[idx]))
            
            return results
            
        except Exception as e:
            logger.error(f"GPU ìœ ì‚¬ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ê¸¸ì´ ì œí•œ
        if len(text) > 500:
            text = text[:500]
        
        return text
    
    def _get_cache_key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def _load_cache(self):
        """ì„ë² ë”© ìºì‹œ ë¡œë“œ"""
        cache_file = os.path.join(self.cache_dir, "gpu_bert_embedding_cache.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    self.embedding_cache = pickle.load(f)
                logger.info(f"ğŸ“‚ GPU BERT ì„ë² ë”© ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
            except Exception as e:
                logger.error(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.embedding_cache = {}
    
    def save_cache(self):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        cache_file = os.path.join(self.cache_dir, "gpu_bert_embedding_cache.pkl")
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(self.embedding_cache, f)
            logger.info(f"ğŸ’¾ GPU BERT ì„ë² ë”© ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_gpu_stats(self) -> Dict[str, any]:
        """GPU í†µê³„ ì •ë³´"""
        if self.device.type == 'cuda':
            allocated = torch.cuda.memory_allocated(self.device) / 1024**2
            cached = torch.cuda.memory_reserved(self.device) / 1024**2
            total = torch.cuda.get_device_properties(self.device).total_memory / 1024**2
            
            return {
                'device': str(self.device),
                'allocated_memory_mb': allocated,
                'cached_memory_mb': cached,
                'total_memory_mb': total,
                'memory_usage_percent': (allocated / total) * 100
            }
        else:
            return {
                'device': str(self.device),
                'allocated_memory_mb': 0,
                'cached_memory_mb': 0,
                'total_memory_mb': 0,
                'memory_usage_percent': 0
            }
    
    def clear_gpu_cache(self):
        """GPU ìºì‹œ ì •ë¦¬"""
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

def test_gpu_performance():
    """GPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª GPU ìµœì í™”ëœ BERT NLP ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "ê²½ì œ ìœ„ê¸°ì™€ ê¸ˆìœµ ì •ì±…ì— ëŒ€í•œ ë¶„ì„",
        "ì •ì¹˜ ê°œí˜ê³¼ ë¯¼ì£¼ì£¼ì˜ ë°œì „",
        "ìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ì™€ ì„ ìˆ˜ë“¤ì˜ í™œì•½",
        "ì‚¬íšŒ ë¬¸ì œì™€ í•´ê²° ë°©ì•ˆ",
        "êµ­ì œ ê´€ê³„ì™€ ì™¸êµ ì •ì±…"
    ] * 50  # 250ê°œ í…ìŠ¤íŠ¸
    
    # GPU ìµœì í™”ëœ BERT NLP ì´ˆê¸°í™”
    bert_nlp = GPUBertNLP()
    
    # GPU í†µê³„ ì¶œë ¥
    gpu_stats = bert_nlp.get_gpu_stats()
    logger.info(f"ğŸ® GPU í†µê³„: {gpu_stats}")
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    start_time = time.time()
    
    # GPU ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    embeddings = bert_nlp.get_embeddings_batch_gpu(test_texts, batch_size=64)
    
    end_time = time.time()
    
    logger.info(f"â±ï¸ GPU ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    logger.info(f"ğŸ“Š ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {len(test_texts)}ê°œ")
    logger.info(f"ğŸš€ ì²˜ë¦¬ ì†ë„: {len(test_texts) / (end_time - start_time):.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    
    # GPU ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query = "ê²½ì œì™€ ê¸ˆìœµì— ê´€í•œ ë‚´ìš©"
    start_time = time.time()
    
    similar_texts = bert_nlp.find_similar_texts_gpu(query, test_texts, top_k=5)
    
    end_time = time.time()
    
    logger.info(f"ğŸ” GPU ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    logger.info(f"ğŸ“‹ ìƒìœ„ ìœ ì‚¬ í…ìŠ¤íŠ¸:")
    for idx, score in similar_texts:
        logger.info(f"   - {test_texts[idx][:50]}... (ì ìˆ˜: {score:.4f})")
    
    # ìµœì¢… GPU í†µê³„
    final_gpu_stats = bert_nlp.get_gpu_stats()
    logger.info(f"ğŸ’¾ ìµœì¢… GPU í†µê³„: {final_gpu_stats}")
    
    # ìºì‹œ ì €ì¥
    bert_nlp.save_cache()
    
    # GPU ìºì‹œ ì •ë¦¬
    bert_nlp.clear_gpu_cache()
    
    logger.info("âœ… GPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    test_gpu_performance()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ìµœì í™”ëœ BERT NLP ì„œë¹„ìŠ¤
- ë°°ì¹˜ ì²˜ë¦¬
- ìºì‹±
- ëª¨ë¸ ìµœì í™”
"""

import os
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import platform
import re
from typing import List, Dict, Tuple, Optional
import logging
import time
import pickle
import hashlib
from concurrent.futures import ThreadPoolExecutor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedBertNLP:
    """
    ìµœì í™”ëœ KoBERT ê¸°ë°˜ NLP ì„œë¹„ìŠ¤
    """
    
    def __init__(self, model_name: str = "skt/kobert-base-v1", cache_dir: str = "cache"):
        """
        ìµœì í™”ëœ BERT NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  BERT ëª¨ë¸ëª…
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
        """
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = None
        self.model = None
        self.sentence_transformer = None
        self.cache_dir = cache_dir
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        
        # ì„ë² ë”© ìºì‹œ
        self.embedding_cache = {}
        
        logger.info(f"ğŸš€ ìµœì í™”ëœ BERT NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (Device: {self.device})")
        self._load_models()
        self._load_cache()
    
    def _load_models(self):
        """BERT ëª¨ë¸ë“¤ ë¡œë“œ (ìµœì í™”)"""
        try:
            # KoBERT í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
            logger.info(f"KoBERT ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            self.model.to(self.device)
            self.model.eval()
            
            # ëª¨ë¸ ìµœì í™”
            if hasattr(torch, 'compile'):
                self.model = torch.compile(self.model)  # PyTorch 2.0 ì»´íŒŒì¼
            
            # Sentence Transformer ë¡œë“œ (ë” ê°€ë²¼ìš´ ëª¨ë¸ ì‚¬ìš©)
            logger.info("Sentence Transformer ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.sentence_transformer = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            
            logger.info("âœ… ëª¨ë“  ìµœì í™”ëœ BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_bert_embedding_optimized(self, text: str) -> np.ndarray:
        """
        ìµœì í™”ëœ BERT ì„ë² ë”© ìƒì„± (ìºì‹œ í™œìš©)
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë¬¸ë§¥ ê¸°ë°˜ ì„ë² ë”© ë²¡í„°
        """
        if not text or not isinstance(text, str):
            return np.zeros(768)
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._get_cache_key(text)
        
        # ìºì‹œì—ì„œ í™•ì¸
        if cache_key in self.embedding_cache:
            return self.embedding_cache[cache_key]
        
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = self._preprocess_text(text)
            
            # í† í¬ë‚˜ì´ì§• (ìµœì í™”)
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=256,  # ê¸¸ì´ ì œí•œìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
                padding=True
            )
            
            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # BERT ì„ë² ë”© ìƒì„± (ìµœì í™”)
            with torch.no_grad():
                outputs = self.model(**inputs)
                # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            result = embedding[0]
            
            # ìºì‹œì— ì €ì¥
            self.embedding_cache[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"BERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros(768)
    
    def get_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[np.ndarray]:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
        
        Args:
            texts: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
        """
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = []
            
            for text in batch_texts:
                if text:
                    embedding = self.get_bert_embedding_optimized(text)
                    batch_embeddings.append(embedding)
                else:
                    batch_embeddings.append(np.zeros(768))
            
            embeddings.extend(batch_embeddings)
        
        return embeddings
    
    def calculate_similarities_batch(self, query_text: str, candidate_texts: List[str]) -> List[float]:
        """
        ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            query_text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            candidate_texts: í›„ë³´ í…ìŠ¤íŠ¸ë“¤
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.get_bert_embedding_optimized(query_text)
        
        # í›„ë³´ í…ìŠ¤íŠ¸ë“¤ ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
        candidate_embeddings = self.get_embeddings_batch(candidate_texts)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for candidate_embedding in candidate_embeddings:
            similarity = cosine_similarity([query_embedding], [candidate_embedding])[0][0]
            similarities.append(float(similarity))
        
        return similarities
    
    def find_similar_texts_optimized(self, query_text: str, candidate_texts: List[str], 
                                   top_k: int = 5, threshold: float = 0.3) -> List[Tuple[int, float]]:
        """
        ìµœì í™”ëœ ìœ ì‚¬ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        
        Args:
            query_text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            candidate_texts: í›„ë³´ í…ìŠ¤íŠ¸ë“¤
            top_k: ë°˜í™˜í•  ìƒìœ„ ê°œìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            (ì¸ë±ìŠ¤, ìœ ì‚¬ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.calculate_similarities_batch(query_text, candidate_texts)
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹ ë¥¸ ì •ë ¬
            similarities_array = np.array(similarities)
            indices = np.argsort(similarities_array)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            
            results = []
            for idx in indices:
                if similarities_array[idx] >= threshold and len(results) < top_k:
                    results.append((idx, similarities_array[idx]))
            
            return results
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ìµœì í™”)"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ê¸¸ì´ ì œí•œ
        if len(text) > 500:
            text = text[:500]
        
        return text
    
    def _get_cache_key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def _load_cache(self):
        """ì„ë² ë”© ìºì‹œ ë¡œë“œ"""
        cache_file = os.path.join(self.cache_dir, "bert_embedding_cache.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    self.embedding_cache = pickle.load(f)
                logger.info(f"ğŸ“‚ BERT ì„ë² ë”© ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
            except Exception as e:
                logger.error(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.embedding_cache = {}
    
    def save_cache(self):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        cache_file = os.path.join(self.cache_dir, "bert_embedding_cache.pkl")
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(self.embedding_cache, f)
            logger.info(f"ğŸ’¾ BERT ì„ë² ë”© ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.embedding_cache.clear()
        cache_file = os.path.join(self.cache_dir, "bert_embedding_cache.pkl")
        if os.path.exists(cache_file):
            os.remove(cache_file)
        logger.info("ğŸ—‘ï¸ BERT ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_cache_stats(self) -> Dict[str, int]:
        """ìºì‹œ í†µê³„"""
        return {
            'cache_size': len(self.embedding_cache),
            'cache_file_size': os.path.getsize(os.path.join(self.cache_dir, "bert_embedding_cache.pkl")) if os.path.exists(os.path.join(self.cache_dir, "bert_embedding_cache.pkl")) else 0
        }

def test_performance():
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª ìµœì í™”ëœ BERT NLP ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "ê²½ì œ ìœ„ê¸°ì™€ ê¸ˆìœµ ì •ì±…ì— ëŒ€í•œ ë¶„ì„",
        "ì •ì¹˜ ê°œí˜ê³¼ ë¯¼ì£¼ì£¼ì˜ ë°œì „",
        "ìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ì™€ ì„ ìˆ˜ë“¤ì˜ í™œì•½",
        "ì‚¬íšŒ ë¬¸ì œì™€ í•´ê²° ë°©ì•ˆ",
        "êµ­ì œ ê´€ê³„ì™€ ì™¸êµ ì •ì±…"
    ] * 20  # 100ê°œ í…ìŠ¤íŠ¸
    
    # ìµœì í™”ëœ BERT NLP ì´ˆê¸°í™”
    bert_nlp = OptimizedBertNLP()
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    start_time = time.time()
    
    # ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    embeddings = bert_nlp.get_embeddings_batch(test_texts, batch_size=32)
    
    end_time = time.time()
    
    logger.info(f"â±ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    logger.info(f"ğŸ“Š ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {len(test_texts)}ê°œ")
    logger.info(f"ğŸš€ ì²˜ë¦¬ ì†ë„: {len(test_texts) / (end_time - start_time):.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    
    # ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query = "ê²½ì œì™€ ê¸ˆìœµì— ê´€í•œ ë‚´ìš©"
    start_time = time.time()
    
    similar_texts = bert_nlp.find_similar_texts_optimized(query, test_texts, top_k=5)
    
    end_time = time.time()
    
    logger.info(f"ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    logger.info(f"ğŸ“‹ ìƒìœ„ ìœ ì‚¬ í…ìŠ¤íŠ¸:")
    for idx, score in similar_texts:
        logger.info(f"   - {test_texts[idx][:50]}... (ì ìˆ˜: {score:.4f})")
    
    # ìºì‹œ í†µê³„
    cache_stats = bert_nlp.get_cache_stats()
    logger.info(f"ğŸ’¾ ìºì‹œ í†µê³„: {cache_stats}")
    
    # ìºì‹œ ì €ì¥
    bert_nlp.save_cache()
    
    logger.info("âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    test_performance()

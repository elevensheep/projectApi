import os
from konlpy.tag import Kkma, Okt, Komoran
from gensim.models import Word2Vec, FastText
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import pandas as pd
from app.services.database import MySQLDatabase
from sklearn.decomposition import PCA
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score
import platform
from matplotlib.patches import Patch
import re
from collections import Counter
import jamo
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class Nlp:
    
    def __init__(self):
        # ì—¬ëŸ¬ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
        self.kkma = Kkma()
        self.okt = Okt()
        self.komoran = Komoran()
        self.model_path = "word2vec.model"
        self.fasttext_path = "fasttext.model"
        self.model = None
        self.fasttext_model = None
        self.db = MySQLDatabase()
        
        # ê¸°ì¡´ ëª¨ë¸ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ
        if os.path.exists(self.model_path):
            self.model = Word2Vec.load(self.model_path)
        if os.path.exists(self.fasttext_path):
            self.fasttext_model = FastText.load(self.fasttext_path)
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê°œì„ """
        if not isinstance(text, str):
            return ""
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì •ê·œí™”
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜',
            'ë•Œ', 'ê³³', 'ë§', 'ì¼', 'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ', 'ê°œ', 'ëª…', 'ê¶Œ',
            'ìª½', 'ì¥', 'íšŒ', 'ë²ˆ', 'ì°¨', 'íšŒ', 'ë²ˆì§¸', 'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸',
            'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€', 'ì–´ë–¤', 'ë¬´ìŠ¨'
        }
        
        return text
    
    def extract_nouns_enhanced(self, texts):
        """í–¥ìƒëœ ëª…ì‚¬ ì¶”ì¶œ - ì—¬ëŸ¬ í˜•íƒœì†Œ ë¶„ì„ê¸° ì¡°í•©"""
        result = []
        
        for text in texts:
            if not isinstance(text, str):
                continue
                
            text = self.preprocess_text(text)
            if not text:
                continue
            
            # ì—¬ëŸ¬ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
            nouns_kkma = set(self.kkma.nouns(text))
            nouns_okt = set(self.okt.nouns(text))
            nouns_komoran = set(self.komoran.nouns(text))
            
            # êµì§‘í•©ìœ¼ë¡œ ì‹ ë¢°ë„ ë†’ì€ ëª…ì‚¬ë§Œ ì¶”ì¶œ
            common_nouns = nouns_kkma & nouns_okt & nouns_komoran
            
            # í•©ì§‘í•©ìœ¼ë¡œ ë” ë§ì€ ëª…ì‚¬ í¬í•¨
            all_nouns = nouns_kkma | nouns_okt | nouns_komoran
            
            # ìµœì¢… ê²°ê³¼: ê³µí†µ ëª…ì‚¬ + ê¸¸ì´ê°€ 2 ì´ìƒì¸ ë‹¨ì–´
            filtered_nouns = []
            for noun in all_nouns:
                if len(noun) > 1 and (noun in common_nouns or len(noun) > 2):
                    filtered_nouns.append(noun)
            
            result.extend(filtered_nouns)
        
        return result
    
    def KonlpyOkt(self, querys):
        """ê¸°ì¡´ ë©”ì„œë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
        return self.extract_nouns_enhanced(querys)
    
    def CreateModel(self, querys, model_type='word2vec'):
        """í–¥ìƒëœ ëª¨ë¸ í•™ìŠµ"""
        sentences = []
        
        for query in querys:
            if isinstance(query, str):
                tokens = self.extract_nouns_enhanced([query])
                if tokens:
                    sentences.append(tokens)
        
        if model_type == 'word2vec':
            # Word2Vec í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
            model = Word2Vec(
                sentences, 
                vector_size=200,  # ë²¡í„° í¬ê¸° ì¦ê°€
                window=5,         # ìœˆë„ìš° í¬ê¸° ì¦ê°€
                min_count=2,      # ìµœì†Œ ë¹ˆë„ ì¦ê°€
                workers=4, 
                sg=1,             # Skip-gram ì‚¬ìš©
                negative=10,      # Negative sampling
                alpha=0.025,      # í•™ìŠµë¥ 
                epochs=20         # ì—í¬í¬ ìˆ˜ ì¦ê°€
            )
            model.save(self.model_path)
            self.model = model
            
        elif model_type == 'fasttext':
            # FastText ëª¨ë¸ (ì„œë¸Œì›Œë“œ ì •ë³´ í™œìš©)
            model = FastText(
                sentences,
                vector_size=200,
                window=5,
                min_count=2,
                workers=4,
                sg=1,
                negative=10,
                alpha=0.025,
                epochs=20
            )
            model.save(self.fasttext_path)
            self.fasttext_model = model
        
        print(f"âœ… {model_type.upper()} ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“Š í•™ìŠµëœ ë¬¸ì¥ ìˆ˜: {len(sentences)}")
        print(f"ğŸ“Š ì–´íœ˜ í¬ê¸°: {len(model.wv.key_to_index)}")
    
    def train_book_model_and_get_tokens(self):
        """í–¥ìƒëœ ì±… ëª¨ë¸ í•™ìŠµ"""
        query = "SELECT books_isbn, books_description FROM tb_books WHERE books_description IS NOT NULL AND books_description != ''"
        fetched_data_book = self.db.fetch_query(query=query)
        df_book = pd.DataFrame(fetched_data_book, columns=["books_isbn", "books_description"])
        
        print(f"ğŸ“š ì´ {len(df_book)}ê°œì˜ ì±… ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ")
        
        # ëª¨ë¸ í•™ìŠµ
        descriptions = df_book['books_description'].tolist()
        self.CreateModel(descriptions, 'word2vec')
        self.CreateModel(descriptions, 'fasttext')
        
        # í† í° ì¶”ì¶œ
        isbn_tokens = {}
        for isbn, description in zip(df_book['books_isbn'], df_book['books_description']):
            tokens = self.extract_nouns_enhanced([description])
            isbn_tokens[isbn] = tokens
            
        return isbn_tokens
    
    def ModelScore(self, word1, word2):
        """í–¥ìƒëœ ìœ ì‚¬ë„ ê³„ì‚°"""
        if self.model is None:
            print("âš ï¸ Word2Vec ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        if self.fasttext_model is None:
            print("âš ï¸ FastText ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # Word2Vec ìœ ì‚¬ë„
        if word1 in self.model.wv and word2 in self.model.wv:
            w2v_similarity = self.model.wv.similarity(word1, word2)
            print(f"ğŸŸ¢ Word2Vec '{word1}' â†” '{word2}': {w2v_similarity:.4f}")
        
        # FastText ìœ ì‚¬ë„
        if word1 in self.fasttext_model.wv and word2 in self.fasttext_model.wv:
            ft_similarity = self.fasttext_model.wv.similarity(word1, word2)
            print(f"ğŸ”µ FastText '{word1}' â†” '{word2}': {ft_similarity:.4f}")
        
        # í‰ê·  ìœ ì‚¬ë„
        similarities = []
        if word1 in self.model.wv and word2 in self.model.wv:
            similarities.append(w2v_similarity)
        if word1 in self.fasttext_model.wv and word2 in self.fasttext_model.wv:
            similarities.append(ft_similarity)
        
        if similarities:
            avg_similarity = np.mean(similarities)
            print(f"ğŸ“Š í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")
    
    def SimilerWord(self, word, topn=5):
        """í–¥ìƒëœ ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°"""
        if self.model is None:
            print("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        results = []
        
        # Word2Vec ê²°ê³¼
        if word in self.model.wv:
            w2v_similar = self.model.wv.most_similar(word, topn=topn)
            results.extend([(word, score, 'Word2Vec') for word, score in w2v_similar])
        
        # FastText ê²°ê³¼
        if self.fasttext_model and word in self.fasttext_model.wv:
            ft_similar = self.fasttext_model.wv.most_similar(word, topn=topn)
            results.extend([(word, score, 'FastText') for word, score in ft_similar])
        
        # ì ìˆ˜ë¡œ ì •ë ¬
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:topn]

    def get_similar_keywords(self, newsData):
        """í–¥ìƒëœ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ë§¤ì¹­"""
        similar_news_data = {}

        for section, keywords in newsData.items():
            similar_keywords = []
            
            for keyword in keywords:
                similar_words = self.SimilerWord(keyword, topn=3)
                
                if similar_words:
                    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë‹¨ì–´ ì„ íƒ
                    best_match = similar_words[0]
                    similar_keywords.append({
                        'original': keyword,
                        'similar': best_match[0],
                        'score': best_match[1],
                        'model': best_match[2]
                    })
            
            similar_news_data[section] = similar_keywords

        return similar_news_data

    def LoadModel(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if os.path.exists(self.model_path):
            try:
                self.model = Word2Vec.load(self.model_path)
                print("ğŸ“¦ Word2Vec ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print("ğŸš¨ Word2Vec ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜:", e)
        
        if os.path.exists(self.fasttext_path):
            try:
                self.fasttext_model = FastText.load(self.fasttext_path)
                print("ğŸ“¦ FastText ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print("ğŸš¨ FastText ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜:", e)
    
    def find_optimal_clusters_elbow(self, word_vectors, max_clusters=20):
        """ì—˜ë³´ìš° ê¸°ë²•ì„ ì‚¬ìš©í•œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°"""
        if len(word_vectors) < 2:
            print("âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 2ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return 2
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ë²”ìœ„ ì„¤ì •
        cluster_range = range(1, min(max_clusters + 1, len(word_vectors)))
        inertias = []
        silhouette_scores = []
        
        print("ğŸ” ì—˜ë³´ìš° ê¸°ë²•ìœ¼ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì°¾ëŠ” ì¤‘...")
        
        for n_clusters in cluster_range:
            if n_clusters == 1:
                # í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œì¼ ë•ŒëŠ” inertia ê³„ì‚°
                inertia = np.sum([np.sum((word_vectors - np.mean(word_vectors, axis=0))**2)])
                inertias.append(inertia)
                silhouette_scores.append(0)  # í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œì¼ ë•ŒëŠ” ì‹¤ë£¨ì—£ ì ìˆ˜ 0
            else:
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                labels = kmeans.fit_predict(word_vectors)
                inertias.append(kmeans.inertia_)
                
                if len(set(labels)) > 1:  # ìµœì†Œ 2ê°œ í´ëŸ¬ìŠ¤í„°
                    silhouette_scores.append(silhouette_score(word_vectors, labels))
                else:
                    silhouette_scores.append(0)
        
        # ì—˜ë³´ìš° í¬ì¸íŠ¸ ì°¾ê¸° (inertiaì˜ ë³€í™”ìœ¨ì´ ê¸‰ê²©íˆ ê°ì†Œí•˜ëŠ” ì§€ì )
        if len(inertias) > 2:
            # 2ì°¨ ë¯¸ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ì—˜ë³´ìš° í¬ì¸íŠ¸ ì°¾ê¸°
            first_derivative = np.diff(inertias)
            second_derivative = np.diff(first_derivative)
            
            # 2ì°¨ ë¯¸ë¶„ì´ ê°€ì¥ í° ì§€ì ì„ ì—˜ë³´ìš° í¬ì¸íŠ¸ë¡œ ì„ íƒ
            if len(second_derivative) > 0:
                elbow_idx = np.argmax(second_derivative) + 2  # +2ëŠ” ì¸ë±ìŠ¤ ë³´ì •
                elbow_clusters = cluster_range[elbow_idx]
            else:
                elbow_clusters = 2
        else:
            elbow_clusters = 2
        
        # ì‹¤ë£¨ì—£ ì ìˆ˜ ê¸°ë°˜ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜
        if len(silhouette_scores) > 1:
            silhouette_optimal = cluster_range[np.argmax(silhouette_scores[1:]) + 1]
        else:
            silhouette_optimal = 2
        
        print(f"ğŸ“Š ì—˜ë³´ìš° ê¸°ë²• ê²°ê³¼: {elbow_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
        print(f"ğŸ“Š ì‹¤ë£¨ì—£ ì ìˆ˜ ê¸°ë°˜: {silhouette_optimal}ê°œ í´ëŸ¬ìŠ¤í„°")
        print(f"ğŸ“Š ìµœê³  ì‹¤ë£¨ì—£ ì ìˆ˜: {max(silhouette_scores):.4f}")
        
        # ì—˜ë³´ìš°ì™€ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        optimal_clusters = min(elbow_clusters, silhouette_optimal)
        
        return optimal_clusters, inertias, silhouette_scores, cluster_range
    
    def plot_elbow_method(self, word_vectors, max_clusters=20, save_path=None):
        """ì—˜ë³´ìš° ê¸°ë²• ì‹œê°í™”"""
        if len(word_vectors) < 2:
            print("âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 2ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # ì—˜ë³´ìš° ê¸°ë²•ìœ¼ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
        optimal_clusters, inertias, silhouette_scores, cluster_range = self.find_optimal_clusters_elbow(word_vectors, max_clusters)
        
        # ê·¸ë˜í”„ ì„¤ì •
        if platform.system() == "Windows":
            plt.rcParams['font.family'] = 'Malgun Gothic'
        else:
            plt.rcParams['font.family'] = 'Nanum Gothic'
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ì—˜ë³´ìš° ê·¸ë˜í”„ (Inertia)
        ax1.plot(cluster_range, inertias, 'bo-', linewidth=2, markersize=8)
        ax1.axvline(x=optimal_clusters, color='red', linestyle='--', linewidth=2, 
                   label=f'ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_clusters}')
        ax1.set_xlabel('í´ëŸ¬ìŠ¤í„° ìˆ˜', fontsize=12)
        ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)
        ax1.set_title('ì—˜ë³´ìš° ê¸°ë²• (Elbow Method)', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # ì‹¤ë£¨ì—£ ì ìˆ˜ ê·¸ë˜í”„
        if len(silhouette_scores) > 1:
            ax2.plot(cluster_range[1:], silhouette_scores[1:], 'go-', linewidth=2, markersize=8)
            silhouette_optimal = cluster_range[np.argmax(silhouette_scores[1:]) + 1]
            ax2.axvline(x=silhouette_optimal, color='red', linestyle='--', linewidth=2,
                       label=f'ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {silhouette_optimal}')
            ax2.set_xlabel('í´ëŸ¬ìŠ¤í„° ìˆ˜', fontsize=12)
            ax2.set_ylabel('Silhouette Score', fontsize=12)
            ax2.set_title('ì‹¤ë£¨ì—£ ì ìˆ˜ (Silhouette Score)', fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3)
            ax2.legend()
        else:
            ax2.text(0.5, 0.5, 'ë°ì´í„° ë¶€ì¡±\n(ìµœì†Œ 2ê°œ í´ëŸ¬ìŠ¤í„° í•„ìš”)', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)
            ax2.set_title('ì‹¤ë£¨ì—£ ì ìˆ˜ (ë°ì´í„° ë¶€ì¡±)', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        # ê·¸ë˜í”„ ì €ì¥
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ì—˜ë³´ìš° ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        
        plt.show()
        
        return optimal_clusters, inertias, silhouette_scores, cluster_range
    
    def find_optimal_clusters(self, word_vectors, max_clusters=20):
        """ê¸°ì¡´ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (í˜¸í™˜ì„± ìœ ì§€)"""
        optimal_clusters, _, _, _ = self.find_optimal_clusters_elbow(word_vectors, max_clusters)
        return optimal_clusters
    
    def VisualizeModel(self, word_list=None, num_clusters=None, method='kmeans', use_elbow=True, show_elbow_plot=True):
        """í–¥ìƒëœ ì‹œê°í™” - ì—˜ë³´ìš° ê¸°ë²• í†µí•©"""
        if self.model is None:
            print("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì„ íƒ
        if word_list is None:
            word_list = list(self.model.wv.key_to_index)[:1000]

        # ë²¡í„° ì¶”ì¶œ
        word_list = [word for word in word_list if word in self.model.wv]
        word_vectors = np.array([self.model.wv[word] for word in word_list])

        # ì°¨ì› ì¶•ì†Œ
        pca = PCA(n_components=min(50, len(word_vectors[0])))
        word_vectors_pca = pca.fit_transform(word_vectors)

        tsne = TSNE(
            n_components=2,
            perplexity=min(30, len(word_vectors) - 1),
            learning_rate=200,
            n_iter=2000,
            random_state=42,
            init='pca'
        )
        reduced_vectors = tsne.fit_transform(word_vectors_pca)

        # í´ëŸ¬ìŠ¤í„°ë§
        if num_clusters is None:
            if use_elbow:
                print("ğŸ” ì—˜ë³´ìš° ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì°¾ëŠ” ì¤‘...")
                if show_elbow_plot:
                    # ì—˜ë³´ìš° ê·¸ë˜í”„ í‘œì‹œ
                    optimal_clusters, inertias, silhouette_scores, cluster_range = self.plot_elbow_method(
                        reduced_vectors, max_clusters=20
                    )
                    num_clusters = optimal_clusters
                else:
                    num_clusters = self.find_optimal_clusters(reduced_vectors)
            else:
                num_clusters = self.find_optimal_clusters(reduced_vectors)
        
        if method == 'kmeans':
            clustering = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        elif method == 'dbscan':
            clustering = DBSCAN(eps=0.5, min_samples=5)
        elif method == 'agglomerative':
            clustering = AgglomerativeClustering(n_clusters=num_clusters)
        
        labels = clustering.fit_predict(reduced_vectors)
        
        # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
        if len(set(labels)) > 1:
            silhouette_avg = silhouette_score(reduced_vectors, labels)
            calinski_avg = calinski_harabasz_score(reduced_vectors, labels)
            print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ - ì‹¤ë£¨ì—£: {silhouette_avg:.4f}, ì¹¼ë¦°ìŠ¤í‚¤: {calinski_avg:.4f}")

        # ì‹œê°í™”
        self._plot_clusters(reduced_vectors, word_list, labels, num_clusters, method)
    
    def find_clusters_with_elbow(self, word_list=None, max_clusters=20, method='kmeans'):
        """ì—˜ë³´ìš° ê¸°ë²•ì„ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§ ì „ìš© í•¨ìˆ˜"""
        if self.model is None:
            print("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None, None, None
        
        # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì„ íƒ
        if word_list is None:
            word_list = list(self.model.wv.key_to_index)[:1000]
        
        # ë²¡í„° ì¶”ì¶œ
        word_list = [word for word in word_list if word in self.model.wv]
        word_vectors = np.array([self.model.wv[word] for word in word_list])
        
        # ì°¨ì› ì¶•ì†Œ
        pca = PCA(n_components=min(50, len(word_vectors[0])))
        word_vectors_pca = pca.fit_transform(word_vectors)
        
        tsne = TSNE(
            n_components=2,
            perplexity=min(30, len(word_vectors) - 1),
            learning_rate=200,
            n_iter=2000,
            random_state=42,
            init='pca'
        )
        reduced_vectors = tsne.fit_transform(word_vectors_pca)
        
        # ì—˜ë³´ìš° ê¸°ë²•ìœ¼ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
        print("ğŸ” ì—˜ë³´ìš° ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì°¾ëŠ” ì¤‘...")
        optimal_clusters, inertias, silhouette_scores, cluster_range = self.plot_elbow_method(
            reduced_vectors, max_clusters=max_clusters
        )
        
        # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        if method == 'kmeans':
            clustering = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)
        elif method == 'dbscan':
            clustering = DBSCAN(eps=0.5, min_samples=5)
        elif method == 'agglomerative':
            clustering = AgglomerativeClustering(n_clusters=optimal_clusters)
        
        labels = clustering.fit_predict(reduced_vectors)
        
        # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
        if len(set(labels)) > 1:
            silhouette_avg = silhouette_score(reduced_vectors, labels)
            calinski_avg = calinski_harabasz_score(reduced_vectors, labels)
            print(f"ğŸ“Š ìµœì¢… í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ - ì‹¤ë£¨ì—£: {silhouette_avg:.4f}, ì¹¼ë¦°ìŠ¤í‚¤: {calinski_avg:.4f}")
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë‹¨ì–´ ê·¸ë£¹í™”
        cluster_groups = {}
        for i, (word, label) in enumerate(zip(word_list, labels)):
            if label not in cluster_groups:
                cluster_groups[label] = []
            cluster_groups[label].append(word)
        
        print(f"âœ… ì´ {len(cluster_groups)}ê°œì˜ í´ëŸ¬ìŠ¤í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for cluster_id, words in cluster_groups.items():
            print(f"ğŸ“Œ í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(words)}ê°œ ë‹¨ì–´")
            if len(words) <= 10:  # ë‹¨ì–´ê°€ 10ê°œ ì´í•˜ë©´ ëª¨ë‘ ì¶œë ¥
                print(f"   ë‹¨ì–´ë“¤: {', '.join(words[:10])}")
            else:  # 10ê°œ ì´ˆê³¼ë©´ ëŒ€í‘œ ë‹¨ì–´ë§Œ ì¶œë ¥
                print(f"   ëŒ€í‘œ ë‹¨ì–´ë“¤: {', '.join(words[:5])}... (ì´ {len(words)}ê°œ)")
        
        return reduced_vectors, labels, cluster_groups
    
    def _plot_clusters(self, vectors, words, labels, num_clusters, method):
        """í´ëŸ¬ìŠ¤í„° ì‹œê°í™”"""
        if platform.system() == "Windows":
            plt.rcParams['font.family'] = 'Malgun Gothic'
        else:
            plt.rcParams['font.family'] = 'Nanum Gothic'

        # ìƒ‰ìƒ ì„¤ì •
        unique_labels = set(labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        color_map = {label: colors[i] for i, label in enumerate(unique_labels)}
        point_colors = [color_map[label] for label in labels]

        plt.figure(figsize=(20, 15))
        scatter = plt.scatter(vectors[:, 0], vectors[:, 1], c=point_colors, alpha=0.7, s=50)

        # ë‹¨ì–´ ë¼ë²¨ (ì¤‘ìš”í•œ ë‹¨ì–´ë§Œ)
        word_freq = Counter(words)
        important_words = [word for word in words if word_freq[word] > 1]
        
        for i, word in enumerate(words):
            if word in important_words:
                plt.annotate(word, (vectors[i, 0] + 0.5, vectors[i, 1] + 0.5), 
                           fontsize=9, alpha=0.8, fontweight='bold')

        # ë²”ë¡€
        legend_elements = []
        for label in unique_labels:
            if label != -1:  # DBSCANì˜ ë…¸ì´ì¦ˆ í´ëŸ¬ìŠ¤í„° ì œì™¸
                cluster_words = [words[i] for i, l in enumerate(labels) if l == label]
                if cluster_words:
                    representative_word = max(cluster_words, key=lambda w: word_freq[w])
                    legend_elements.append(
                        plt.Line2D([0], [0], marker='o', color='w', 
                                  markerfacecolor=color_map[label], 
                                  markersize=10, label=f"í´ëŸ¬ìŠ¤í„° {label}: {representative_word}")
                    )

        plt.legend(handles=legend_elements, title="ğŸ“Œ í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ë‹¨ì–´", 
                  loc="upper left", bbox_to_anchor=(0, 1), fontsize=10)
        
        plt.title(f"Word2Vec ë‹¨ì–´ ë²¡í„° í´ëŸ¬ìŠ¤í„°ë§ ({method.upper()})", fontsize=16)
        plt.xlabel("t-SNE-1", fontsize=12)
        plt.ylabel("t-SNE-2", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
import os
from konlpy.tag import Kkma
from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import pandas as pd
from app.services.database import MySQLDatabase
from sklearn.decomposition import PCA
import numpy as np
from sklearn.cluster import KMeans
import platform
from matplotlib.patches import Patch

class Nlp:
    
    def __init__(self):
        self.kkma = Kkma()  # Okt ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.model_path = "word2vec.model"
        self.model = None
        self.db = MySQLDatabase()
        # ê¸°ì¡´ ëª¨ë¸ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ
        if os.path.exists(self.model_path):
            self.model = Word2Vec.load(self.model_path)
    
    def KonlpyOkt(self, querys):
        """ë¬¸ì¥ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (í•œ ê¸€ì ë‹¨ì–´ ì œê±°)"""
        result = []
        for query in querys:
            if isinstance(query, str):
                nouns = self.kkma.nouns(query)
                filtered_noun = [word for word in nouns if len(word) > 1]
                if filtered_noun:
                    result.extend(filtered_noun)
        return result
    
    def CreateModel(self, querys):
        """Word2Vec ëª¨ë¸ í•™ìŠµ í›„ ì €ì¥"""
        sentences = []
        for query in querys:
            if isinstance(query, str):
                # ê° queryë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ KonlpyOktì— ì „ë‹¬í•˜ë©´, í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
                tokens = self.KonlpyOkt([query])
                if tokens:  # í† í°ì´ ì¡´ì¬í•  ê²½ìš°ì—ë§Œ ì¶”ê°€
                    sentences.append(tokens)
                    
        # sentencesëŠ” ì´ì œ ê° ë¬¸ì¥ì˜ í† í° ë¦¬ìŠ¤íŠ¸ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ì„
        model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, workers=4, sg=0)
        model.save(self.model_path)
        self.model = model  # ëª¨ë¸ ì—…ë°ì´íŠ¸
        print("âœ… Word2Vec ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")

    def train_book_model_and_get_tokens(self):
        """
        ì±… ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ê° ì±…ì˜ descriptionì—ì„œ KonlpyOkt í•¨ìˆ˜ë¥¼ í†µí•´ 
        ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ISBNë³„ë¡œ ë§¤í•‘í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        ë°˜í™˜ê°’: {isbn: tokens_list}
        """
        
        query = "SELECT books_isbn, books_description FROM tb_books"
        fetched_data_book = self.db.fetch_query(query=query)
        df_book = pd.DataFrame(fetched_data_book, columns=["books_isbn", "books_description"])
        print(df_book.info())
        
        # ëª¨ë¸ í•™ìŠµ (ì±…ì˜ descriptionì„ ì´ìš©í•˜ì—¬ Word2Vec ëª¨ë¸ ìƒì„±)
        descriptions = df_book['books_description'].tolist()
        self.CreateModel(descriptions)
        
        isbn_tokens = {}
        for isbn, description in zip(df_book['books_isbn'], df_book['books_description']):
            tokens = self.KonlpyOkt([description])
            isbn_tokens[isbn] = tokens
            
        return isbn_tokens
    
    def ModelScore(self, word1, word2):
        """ë‘ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
        if self.model is None:
            print("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € CreateModelì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        if word1 in self.model.wv and word2 in self.model.wv:
            similarity = self.model.wv.similarity(word1, word2)
            print(f"ğŸŸ¢ '{word1}'ê³¼(ì™€) '{word2}'ì˜ ìœ ì‚¬ë„: {similarity:.4f}")
        else:
            print(f"âš ï¸ '{word1}' ë˜ëŠ” '{word2}'ê°€ ëª¨ë¸ì— ì—†ìŠµë‹ˆë‹¤.")
    
    def SimilerWord(self, word):
        """íŠ¹ì • ë‹¨ì–´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ë°˜í™˜"""
        if self.model is None:
            print("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € CreateModelì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return []

        if word in self.model.wv:
            similar_words = self.model.wv.most_similar(word, topn=2)
            result = [(similar_word, score) for similar_word, score in similar_words]  # âœ… ê²°ê³¼ ì €ì¥
            return result  # âœ… ê²°ê³¼ ë°˜í™˜
        else:
            print(f"âš ï¸ '{word}'ê°€ ëª¨ë¸ì— ì—†ìŠµë‹ˆë‹¤.")
            return []

    def get_similar_keywords(self, newsData):
        """
        newsDataì˜ ê° í‚¤ì›Œë“œì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì•„ sectionë³„ë¡œ ë°˜í™˜
        """
        similar_news_data = {}  # ìµœì¢… ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬

        for section, keywords in newsData.items():
            similar_keywords = []
            
            for keyword in keywords:
                similar_words = self.SimilerWord(keyword)  # âœ… ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°
                
                # âœ… ìœ ì‚¬í•œ ë‹¨ì–´ê°€ ìˆì„ ê²½ìš°, ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ê°€ì ¸ì˜¤ê¸° (top1)
                if similar_words:
                    similar_keywords.append(similar_words[0][0])  # ë‹¨ì–´ë§Œ ì €ì¥
            
            # âœ… ê²°ê³¼ ì €ì¥
            similar_news_data[section] = similar_keywords

        return similar_news_data  # âœ… sectionë³„ ìœ ì‚¬ í‚¤ì›Œë“œ ë°˜í™˜

    def LoadModel(self):
        if not os.path.exists(self.model_path):
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            return

        try:
            self.model = Word2Vec.load(self.model_path)
            print("ğŸ“¦ Word2Vec ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print("ğŸš¨ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:", e)
                
    def VisualizeModel(self, word_list=None, num_clusters=12):
        """Word2Vec ëª¨ë¸ì˜ ë‹¨ì–´ ë²¡í„°ë¥¼ 2Dë¡œ ì‹œê°í™” (t-SNE + KMeans + ëŒ€í‘œ í‚¤ì›Œë“œ + ë²”ë¡€)"""

        if self.model is None:
            print("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € CreateModelì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì„ íƒ
        if word_list is None:
            word_list = self.model.wv.index_to_key[:1000]

        # ë²¡í„° ì¶”ì¶œ
        word_list = [word for word in word_list if word in self.model.wv]
        word_vectors = np.array([self.model.wv[word] for word in word_list])

        # PCAë¡œ ì¶•ì†Œ í›„ t-SNE
        pca = PCA(n_components=50)
        word_vectors_pca = pca.fit_transform(word_vectors)

        tsne = TSNE(
            n_components=2,
            perplexity=10,
            learning_rate=100,
            n_iter=1500,
            random_state=42,
            init='pca'
        )
        reduced_vectors = tsne.fit_transform(word_vectors_pca)

        # KMeans êµ°ì§‘í™”
        kmeans = KMeans(n_clusters=num_clusters, random_state=42)
        labels = kmeans.fit_predict(reduced_vectors)
        centers = kmeans.cluster_centers_

        # ê° êµ°ì§‘ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ ì„ ì • (êµ°ì§‘ ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´)
        cluster_keywords = {}
        for i in range(num_clusters):
            indices = np.where(labels == i)[0]
            if len(indices) == 0:
                continue
            cluster_vecs = reduced_vectors[indices]
            center = centers[i]
            distances = np.linalg.norm(cluster_vecs - center, axis=1)
            closest_idx = indices[np.argmin(distances)]
            cluster_keywords[i] = (word_list[closest_idx], center)

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        if platform.system() == "Windows":
            plt.rcParams['font.family'] = 'Malgun Gothic'
        else:
            plt.rcParams['font.family'] = 'Nanum Gothic'

        # ìƒ‰ìƒ ì„¤ì •
        color_map = plt.cm.tab10(np.linspace(0, 1, num_clusters))
        point_colors = [color_map[label] for label in labels]

        # ì‹œê°í™”
        plt.figure(figsize=(22, 15))
        plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=point_colors, alpha=0.6, edgecolors="k")

        # ê° ë‹¨ì–´ ë¼ë²¨
        for i, word in enumerate(word_list):
            plt.annotate(word, (reduced_vectors[i, 0] + 0.3, reduced_vectors[i, 1] + 0.3), fontsize=10, alpha=0.6)

        # í‚¤ì›Œë“œ í‘œì‹œ
        legend_elements = []
        for cluster_idx, (keyword, center) in cluster_keywords.items():
            legend_elements.append(Patch(facecolor=color_map[cluster_idx], label=f"{keyword}"))

        # ì™¼ìª½ ìƒë‹¨ì— ë²”ë¡€ ì¶”ê°€
        plt.legend(handles=legend_elements, title="ğŸ“Œ êµ°ì§‘ ëŒ€í‘œ í‚¤ì›Œë“œ", loc="upper left", bbox_to_anchor=(-0.1, 1.03),
                fontsize=12, title_fontsize=13)

        plt.title("Word2Vec ë‹¨ì–´ ë²¡í„° êµ°ì§‘ ì‹œê°í™” (t-SNE + KMeans)", fontsize=18)
        plt.xlabel("t-SNE-1", fontsize=14)
        plt.ylabel("t-SNE-2", fontsize=14)
        plt.grid(True)
        plt.tight_layout()
        plt.show()
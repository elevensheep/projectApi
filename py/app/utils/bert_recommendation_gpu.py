#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GPU ìµœì í™”ëœ BERT ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ
- CUDA ê°€ì†
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.bert_nlp_gpu_optimized import GPUBertNLP
from services.database import MySQLDatabase
from datetime import datetime
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import logging
from typing import List, Dict, Tuple, Optional
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import hashlib
import torch

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GPUBertRecommendationSystem:
    """
    GPU ìµœì í™”ëœ BERT ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, cache_dir: str = "cache", batch_size: int = 128, max_workers: int = 2):
        """GPU ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.bert_nlp = GPUBertNLP()
        self.db = MySQLDatabase()
        self.cache_dir = cache_dir
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.use_gpu = torch.cuda.is_available()
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        
        # ì„ë² ë”© ìºì‹œ
        self.embedding_cache = {}
        
        logger.info(f"ğŸš€ GPU ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - GPU ì‚¬ìš©: {self.use_gpu}")
        logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        logger.info(f"   - ì›Œì»¤ ìˆ˜: {max_workers}")
        
        # GPU í†µê³„ ì¶œë ¥
        gpu_stats = self.bert_nlp.get_gpu_stats()
        logger.info(f"   - GPU í†µê³„: {gpu_stats}")
    
    def recommend_books_by_context_gpu(self, news_data: dict) -> Dict[str, List[Tuple[str, float]]]:
        """
        GPU ìµœì í™”ëœ ë¬¸ë§¥ ê¸°ë°˜ ë„ì„œ ì¶”ì²œ
        
        Args:
            news_data: ë‰´ìŠ¤ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œ ë„ì„œ ë¦¬ìŠ¤íŠ¸
        """
        start_time = time.time()
        logger.info("ğŸ§  GPU ìµœì í™”ëœ ë¬¸ë§¥ ê¸°ë°˜ ë„ì„œ ì¶”ì²œ ì‹œì‘")
        
        # 1. ë„ì„œ ë°ì´í„° ë°°ì¹˜ ë¡œë“œ
        books_data = self._load_books_batch()
        logger.info(f"ğŸ“š {len(books_data['isbn'])}ê¶Œì˜ ë„ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        # 2. ë„ì„œ ì„ë² ë”© GPU ë°°ì¹˜ ìƒì„± (ìºì‹œ í™œìš©)
        book_embeddings = self._get_book_embeddings_gpu_batch(books_data['description'])
        logger.info(f"ğŸ” {len(book_embeddings)}ê°œì˜ ë„ì„œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
        recommendations = {}
        
        # 3. ì¹´í…Œê³ ë¦¬ë³„ ë³‘ë ¬ ì²˜ë¦¬ (GPU ì‚¬ìš© ì‹œ ì›Œì»¤ ìˆ˜ ì¤„ì„)
        if self.use_gpu:
            # GPU ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ê²½í•©ì„ í”¼í•˜ê¸° ìœ„í•´ ì›Œì»¤ ìˆ˜ ì¤„ì„
            actual_workers = min(self.max_workers, 2)
        else:
            actual_workers = self.max_workers
        
        with ThreadPoolExecutor(max_workers=actual_workers) as executor:
            future_to_category = {}
            
            for category, keywords in news_data.items():
                future = executor.submit(
                    self._process_category_gpu,
                    category, keywords, book_embeddings, books_data
                )
                future_to_category[future] = category
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_category):
                category = future_to_category[future]
                try:
                    category_recommendations = future.result()
                    recommendations[category] = category_recommendations
                    logger.info(f"âœ… {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: {len(category_recommendations)}ê¶Œ")
                except Exception as e:
                    logger.error(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    recommendations[category] = []
        
        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ ì „ì²´ ì¶”ì²œ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        
        # GPU í†µê³„ ì¶œë ¥
        final_gpu_stats = self.bert_nlp.get_gpu_stats()
        logger.info(f"ğŸ’¾ ìµœì¢… GPU í†µê³„: {final_gpu_stats}")
        
        return recommendations
    
    def _load_books_batch(self) -> Dict[str, List]:
        """ë„ì„œ ë°ì´í„° ë°°ì¹˜ ë¡œë“œ"""
        query = """
            SELECT books_isbn, books_title, books_description 
            FROM tb_books 
            WHERE books_description IS NOT NULL AND books_description != ''
            LIMIT 2000  -- GPU ì‚¬ìš© ì‹œ ë” ë§ì€ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
        """
        books = self.db.fetch_query(query)
        
        return {
            'isbn': [book[0] for book in books],
            'title': [book[1] for book in books],
            'description': [book[2] for book in books]
        }
    
    def _get_book_embeddings_gpu_batch(self, descriptions: List[str]) -> List[np.ndarray]:
        """ë„ì„œ ì„ë² ë”© GPU ë°°ì¹˜ ìƒì„± (ìºì‹œ í™œìš©)"""
        embeddings = []
        
        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if self.use_gpu:
            gpu_stats = self.bert_nlp.get_gpu_stats()
            gpu_memory_gb = gpu_stats['total_memory_mb'] / 1024
            
            if gpu_memory_gb < 4:  # 4GB ë¯¸ë§Œ
                batch_size = min(self.batch_size, 32)
            elif gpu_memory_gb < 8:  # 8GB ë¯¸ë§Œ
                batch_size = min(self.batch_size, 64)
            else:  # 8GB ì´ìƒ
                batch_size = min(self.batch_size, 128)
        else:
            batch_size = min(self.batch_size, 32)
        
        logger.info(f"ğŸ“¦ GPU ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        for i in range(0, len(descriptions), batch_size):
            batch_descriptions = descriptions[i:i+batch_size]
            
            # GPU ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            batch_embeddings = self.bert_nlp.get_embeddings_batch_gpu(batch_descriptions, batch_size)
            embeddings.extend(batch_embeddings)
            
            # ì§„í–‰ë¥  ì¶œë ¥
            progress = (i + batch_size) / len(descriptions) * 100
            logger.info(f"   ì§„í–‰ë¥ : {min(progress, 100):.1f}%")
        
        return embeddings
    
    def _process_category_gpu(self, category: str, keywords: List[str], 
                            book_embeddings: List[np.ndarray], 
                            books_data: Dict[str, List]) -> List[Tuple[str, float]]:
        """ì¹´í…Œê³ ë¦¬ë³„ GPU ìµœì í™”ëœ ì²˜ë¦¬"""
        category_recommendations = []
        
        for keyword in keywords:
            # í‚¤ì›Œë“œ ì„ë² ë”© ìƒì„±
            context = f"{category} ê´€ë ¨ {keyword}ì— ëŒ€í•œ ë‚´ìš©"
            context_embedding = self.bert_nlp.get_bert_embedding_gpu(context)
            
            # GPU ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self._calculate_similarities_gpu_batch(context_embedding, book_embeddings)
            
            # ìƒìœ„ ì¶”ì²œ ë„ì„œ ì„ íƒ
            top_books = self._get_top_recommendations_gpu(
                similarities, books_data['isbn'], books_data['title'], 
                threshold=0.3, top_k=5
            )
            
            category_recommendations.extend(top_books)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ í†µí•©
        return self._merge_recommendations_gpu(category_recommendations)
    
    def _calculate_similarities_gpu_batch(self, query_embedding: np.ndarray, 
                                        book_embeddings: List[np.ndarray]) -> List[float]:
        """GPU ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if self.use_gpu and len(book_embeddings) > 1000:
            # GPUì—ì„œ ëŒ€ìš©ëŸ‰ ìœ ì‚¬ë„ ê³„ì‚°
            return self.bert_nlp._calculate_similarities_gpu(query_embedding, book_embeddings)
        else:
            # CPUì—ì„œ ê³„ì‚° (ì†Œìš©ëŸ‰ ë°ì´í„°)
            similarities = []
            for book_embedding in book_embeddings:
                similarity = cosine_similarity([query_embedding], [book_embedding])[0][0]
                similarities.append(float(similarity))
            return similarities
    
    def _get_top_recommendations_gpu(self, similarities: List[float], 
                                   isbns: List[str], titles: List[str], 
                                   threshold: float = 0.3, 
                                   top_k: int = 5) -> List[Tuple[str, float, str]]:
        """GPU ìµœì í™”ëœ ìƒìœ„ ì¶”ì²œ ë„ì„œ ì„ íƒ"""
        # numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹ ë¥¸ ì •ë ¬
        similarities_array = np.array(similarities)
        indices = np.argsort(similarities_array)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        
        recommendations = []
        for idx in indices:
            if similarities_array[idx] >= threshold and len(recommendations) < top_k:
                recommendations.append((isbns[idx], similarities_array[idx], titles[idx]))
        
        return recommendations
    
    def _merge_recommendations_gpu(self, recommendations: List[Tuple[str, float, str]]) -> List[Tuple[str, float]]:
        """GPU ìµœì í™”ëœ ì¶”ì²œ ê²°ê³¼ í†µí•©"""
        merged = {}
        
        for isbn, score, title in recommendations:
            if isbn not in merged or score > merged[isbn]:
                merged[isbn] = score
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_recs = sorted(merged.items(), key=lambda x: x[1], reverse=True)
        
        return sorted_recs[:10]  # ìƒìœ„ 10ê°œ ë°˜í™˜
    
    def save_recommendations_to_db(self, recommendations: Dict[str, List[Tuple[str, float]]], 
                                 method: str = "gpu_bert"):
        """ì¶”ì²œ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        logger.info(f"ğŸ’¾ ì¶”ì²œ ê²°ê³¼ DB ì €ì¥ ì‹œì‘ (ë°©ë²•: {method})")
        
        try:
            # ê¸°ì¡´ ì¶”ì²œ ë°ì´í„° ì‚­ì œ
            delete_query = "DELETE FROM tb_recommend WHERE method = %s"
            self.db.execute_query(delete_query, (method,))
            
            # ìƒˆë¡œìš´ ì¶”ì²œ ë°ì´í„° ì‚½ì…
            insert_query = """
                INSERT INTO tb_recommend (news_keyword, books_isbn, similarity_score, method, created_at)
                VALUES (%s, %s, %s, %s, %s)
            """
            
            total_inserted = 0
            for category, recs in recommendations.items():
                for isbn, score in recs:
                    self.db.execute_query(insert_query, (
                        category, isbn, score, method, datetime.now()
                    ))
                    total_inserted += 1
            
            logger.info(f"âœ… ì¶”ì²œ ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ: {total_inserted}ê°œ")
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ì²œ ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _get_cache_key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def save_cache(self):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        cache_file = os.path.join(self.cache_dir, "gpu_embedding_cache.pkl")
        with open(cache_file, 'wb') as f:
            pickle.dump(self.embedding_cache, f)
        logger.info(f"ğŸ’¾ GPU ì„ë² ë”© ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
    
    def load_cache(self):
        """ì„ë² ë”© ìºì‹œ ë¡œë“œ"""
        cache_file = os.path.join(self.cache_dir, "gpu_embedding_cache.pkl")
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                self.embedding_cache = pickle.load(f)
            logger.info(f"ğŸ“‚ GPU ì„ë² ë”© ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.embedding_cache.clear()
        cache_file = os.path.join(self.cache_dir, "gpu_embedding_cache.pkl")
        if os.path.exists(cache_file):
            os.remove(cache_file)
        logger.info("ğŸ—‘ï¸ GPU ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.save_cache()
        self.bert_nlp.save_cache()
        self.bert_nlp.clear_gpu_cache()
        self.db.close()
        logger.info("ğŸ”š GPU ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì¢…ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ GPU ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì‹œì‘")
    
    try:
        # ì¤‘ë³µ ë°ì´í„° ì²´í¬
        from services.duplicate_checker import DuplicateDataChecker
        
        checker = DuplicateDataChecker()
        should_skip = checker.should_skip_processing()
        checker.close()
        
        if should_skip:
            logger.info("â­ï¸  ì˜¤ëŠ˜ì ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # í¬ë¡¤ëŸ¬ë¡œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        from services.crowling import Crowling
        crawler = Crowling()
        
        # ë‰´ìŠ¤ ì œëª© ê°€ì ¸ì˜¤ê¸°
        logger.info("ğŸ“¡ ì¤‘ì•™ì¼ë³´ ë‰´ìŠ¤ ì œëª© í¬ë¡¤ë§ ì¤‘...")
        news_titles = crawler.get_news_titles()
        logger.info(f"âœ… ë‰´ìŠ¤ ì œëª© í¬ë¡¤ë§ ì™„ë£Œ: {len(news_titles)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        # GPU ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        recommender = GPUBertRecommendationSystem(
            batch_size=128,  # GPU ì‚¬ìš© ì‹œ ë” í° ë°°ì¹˜ í¬ê¸°
            max_workers=2    # GPU ì‚¬ìš© ì‹œ ì›Œì»¤ ìˆ˜ ì¤„ì„
        )
        
        # ìºì‹œ ë¡œë“œ
        recommender.load_cache()
        
        # GPU ìµœì í™”ëœ ì¶”ì²œ ì‹¤í–‰
        logger.info("ğŸ”„ GPU ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œì‘...")
        start_time = time.time()
        
        recommendations = recommender.recommend_books_by_context_gpu(news_titles)
        
        total_time = time.time() - start_time
        logger.info(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ê²°ê³¼ ì¶œë ¥
        total_recommendations = 0
        for category, recs in recommendations.items():
            logger.info(f"\nğŸ“š {category} ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ë„ì„œ:")
            for isbn, score in recs[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                logger.info(f"   - {isbn}: {score:.4f}")
            total_recommendations += len(recs)
        
        logger.info(f"\nğŸ“Š ì´ ì¶”ì²œ ìˆ˜: {total_recommendations}ê°œ")
        
        # DBì— ì €ì¥
        recommender.save_recommendations_to_db(recommendations, "gpu_bert")
        
    except Exception as e:
        logger.error(f"âŒ GPU ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if 'recommender' in locals():
            recommender.close()

if __name__ == "__main__":
    main()

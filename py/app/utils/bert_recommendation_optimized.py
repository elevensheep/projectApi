#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ìµœì í™”ëœ BERT ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
- ìºì‹± ì‹œìŠ¤í…œ
- ë³‘ë ¬ ì²˜ë¦¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.bert_nlp import BertNLP
from services.database import MySQLDatabase
from datetime import datetime
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import logging
from typing import List, Dict, Tuple, Optional
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedBertRecommendationSystem:
    """
    ìµœì í™”ëœ BERT ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, cache_dir: str = "cache", batch_size: int = 64, max_workers: int = 4):
        """ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.bert_nlp = BertNLP()
        self.db = MySQLDatabase()
        self.cache_dir = cache_dir
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        
        # ì„ë² ë”© ìºì‹œ
        self.embedding_cache = {}
        
        logger.info(f"ğŸš€ ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (ë°°ì¹˜í¬ê¸°: {batch_size}, ì›Œì»¤: {max_workers})")
    
    def recommend_books_by_context_optimized(self, news_data: dict) -> Dict[str, List[Tuple[str, float]]]:
        """
        ìµœì í™”ëœ ë¬¸ë§¥ ê¸°ë°˜ ë„ì„œ ì¶”ì²œ
        
        Args:
            news_data: ë‰´ìŠ¤ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œ ë„ì„œ ë¦¬ìŠ¤íŠ¸
        """
        start_time = time.time()
        logger.info("ğŸ§  ìµœì í™”ëœ ë¬¸ë§¥ ê¸°ë°˜ ë„ì„œ ì¶”ì²œ ì‹œì‘")
        
        # 1. ë„ì„œ ë°ì´í„° ë°°ì¹˜ ë¡œë“œ
        books_data = self._load_books_batch()
        logger.info(f"ğŸ“š {len(books_data['isbn'])}ê¶Œì˜ ë„ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        # 2. ë„ì„œ ì„ë² ë”© ë°°ì¹˜ ìƒì„± (ìºì‹œ í™œìš©)
        book_embeddings = self._get_book_embeddings_batch(books_data['description'])
        logger.info(f"ğŸ” {len(book_embeddings)}ê°œì˜ ë„ì„œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
        recommendations = {}
        
        # 3. ì¹´í…Œê³ ë¦¬ë³„ ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_category = {}
            
            for category, keywords in news_data.items():
                future = executor.submit(
                    self._process_category_optimized,
                    category, keywords, book_embeddings, books_data
                )
                future_to_category[future] = category
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_category):
                category = future_to_category[future]
                try:
                    category_recommendations = future.result()
                    recommendations[category] = category_recommendations
                    logger.info(f"âœ… {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: {len(category_recommendations)}ê¶Œ")
                except Exception as e:
                    logger.error(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    recommendations[category] = []
        
        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ ì „ì²´ ì¶”ì²œ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        
        return recommendations
    
    def _load_books_batch(self) -> Dict[str, List]:
        """ë„ì„œ ë°ì´í„° ë°°ì¹˜ ë¡œë“œ"""
        query = """
            SELECT books_isbn, books_title, books_description 
            FROM tb_books 
            WHERE books_description IS NOT NULL AND books_description != ''
            LIMIT 1000  -- ì„±ëŠ¥ì„ ìœ„í•´ ì œí•œ
        """
        books = self.db.fetch_query(query)
        
        return {
            'isbn': [book[0] for book in books],
            'title': [book[1] for book in books],
            'description': [book[2] for book in books]
        }
    
    def _get_book_embeddings_batch(self, descriptions: List[str]) -> List[np.ndarray]:
        """ë„ì„œ ì„ë² ë”© ë°°ì¹˜ ìƒì„± (ìºì‹œ í™œìš©)"""
        embeddings = []
        
        for i in range(0, len(descriptions), self.batch_size):
            batch_descriptions = descriptions[i:i+self.batch_size]
            batch_embeddings = []
            
            for desc in batch_descriptions:
                if desc:
                    # ìºì‹œ í‚¤ ìƒì„±
                    cache_key = self._get_cache_key(desc)
                    
                    if cache_key in self.embedding_cache:
                        # ìºì‹œì—ì„œ ë¡œë“œ
                        embedding = self.embedding_cache[cache_key]
                    else:
                        # ìƒˆë¡œ ìƒì„±
                        embedding = self.bert_nlp.get_bert_embedding(desc)
                        self.embedding_cache[cache_key] = embedding
                    
                    batch_embeddings.append(embedding)
                else:
                    batch_embeddings.append(np.zeros(768))
            
            embeddings.extend(batch_embeddings)
        
        return embeddings
    
    def _process_category_optimized(self, category: str, keywords: List[str], 
                                  book_embeddings: List[np.ndarray], 
                                  books_data: Dict[str, List]) -> List[Tuple[str, float]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ìµœì í™”ëœ ì²˜ë¦¬"""
        category_recommendations = []
        
        for keyword in keywords:
            # í‚¤ì›Œë“œ ì„ë² ë”© ìƒì„±
            context = f"{category} ê´€ë ¨ {keyword}ì— ëŒ€í•œ ë‚´ìš©"
            context_embedding = self.bert_nlp.get_bert_embedding(context)
            
            # ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self._calculate_similarities_batch(context_embedding, book_embeddings)
            
            # ìƒìœ„ ì¶”ì²œ ë„ì„œ ì„ íƒ
            top_books = self._get_top_recommendations_optimized(
                similarities, books_data['isbn'], books_data['title'], 
                threshold=0.3, top_k=5
            )
            
            category_recommendations.extend(top_books)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ í†µí•©
        return self._merge_recommendations_optimized(category_recommendations)
    
    def _calculate_similarities_batch(self, query_embedding: np.ndarray, 
                                    book_embeddings: List[np.ndarray]) -> List[float]:
        """ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        similarities = []
        
        for book_embedding in book_embeddings:
            similarity = cosine_similarity([query_embedding], [book_embedding])[0][0]
            similarities.append(float(similarity))
        
        return similarities
    
    def _get_top_recommendations_optimized(self, similarities: List[float], 
                                         isbns: List[str], titles: List[str], 
                                         threshold: float = 0.3, 
                                         top_k: int = 5) -> List[Tuple[str, float, str]]:
        """ìµœì í™”ëœ ìƒìœ„ ì¶”ì²œ ë„ì„œ ì„ íƒ"""
        # numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹ ë¥¸ ì •ë ¬
        similarities_array = np.array(similarities)
        indices = np.argsort(similarities_array)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        
        recommendations = []
        for idx in indices:
            if similarities_array[idx] >= threshold and len(recommendations) < top_k:
                recommendations.append((isbns[idx], similarities_array[idx], titles[idx]))
        
        return recommendations
    
    def _merge_recommendations_optimized(self, recommendations: List[Tuple[str, float, str]]) -> List[Tuple[str, float]]:
        """ìµœì í™”ëœ ì¶”ì²œ ê²°ê³¼ í†µí•©"""
        merged = {}
        
        for isbn, score, title in recommendations:
            if isbn not in merged or score > merged[isbn]:
                merged[isbn] = score
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_recs = sorted(merged.items(), key=lambda x: x[1], reverse=True)
        
        return sorted_recs[:10]  # ìƒìœ„ 10ê°œ ë°˜í™˜
    
    def _get_cache_key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def save_cache(self):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        cache_file = os.path.join(self.cache_dir, "embedding_cache.pkl")
        with open(cache_file, 'wb') as f:
            pickle.dump(self.embedding_cache, f)
        logger.info(f"ğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
    
    def load_cache(self):
        """ì„ë² ë”© ìºì‹œ ë¡œë“œ"""
        cache_file = os.path.join(self.cache_dir, "embedding_cache.pkl")
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                self.embedding_cache = pickle.load(f)
            logger.info(f"ğŸ“‚ ì„ë² ë”© ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.embedding_cache.clear()
        cache_file = os.path.join(self.cache_dir, "embedding_cache.pkl")
        if os.path.exists(cache_file):
            os.remove(cache_file)
        logger.info("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.save_cache()
        self.db.close()
        logger.info("ğŸ”š ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì¢…ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì‹œì‘")
    
    try:
        # í¬ë¡¤ëŸ¬ë¡œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        from services.crowling import Crowling
        crawler = Crowling()
        
        # ë‰´ìŠ¤ ì œëª© ê°€ì ¸ì˜¤ê¸°
        logger.info("ğŸ“¡ ì¤‘ì•™ì¼ë³´ ë‰´ìŠ¤ ì œëª© í¬ë¡¤ë§ ì¤‘...")
        news_titles = crawler.get_news_titles()
        logger.info(f"âœ… ë‰´ìŠ¤ ì œëª© í¬ë¡¤ë§ ì™„ë£Œ: {len(news_titles)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        # ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        recommender = OptimizedBertRecommendationSystem(
            batch_size=64,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
            max_workers=4   # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
        )
        
        # ìºì‹œ ë¡œë“œ
        recommender.load_cache()
        
        # ìµœì í™”ëœ ì¶”ì²œ ì‹¤í–‰
        logger.info("ğŸ”„ ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œì‘...")
        start_time = time.time()
        
        recommendations = recommender.recommend_books_by_context_optimized(news_titles)
        
        total_time = time.time() - start_time
        logger.info(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ê²°ê³¼ ì¶œë ¥
        total_recommendations = 0
        for category, recs in recommendations.items():
            logger.info(f"\nğŸ“š {category} ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ë„ì„œ:")
            for isbn, score in recs[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                logger.info(f"   - {isbn}: {score:.4f}")
            total_recommendations += len(recs)
        
        logger.info(f"\nğŸ“Š ì´ ì¶”ì²œ ìˆ˜: {total_recommendations}ê°œ")
        
        # DBì— ì €ì¥
        recommender.save_recommendations_to_db(recommendations, "optimized_bert")
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™”ëœ BERT ì¶”ì²œ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if 'recommender' in locals():
            recommender.close()

if __name__ == "__main__":
    main()

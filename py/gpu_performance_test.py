#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- GPU vs CPU ì„±ëŠ¥ ë¹„êµ
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ ì¸¡ì •
"""

import sys
import os
import time
import psutil
import gc

# app í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
app_dir = os.path.join(current_dir, 'app')
sys.path.append(app_dir)

from services.bert_nlp import BertNLP
from services.bert_nlp_gpu_optimized import GPUBertNLP
from services.crowling import Crowling
import logging
import torch

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_memory_usage():
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

def test_cpu_bert():
    """CPU BERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª CPU BERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê¸°
    initial_memory = get_memory_usage()
    
    # CPU BERT ì´ˆê¸°í™”
    start_time = time.time()
    bert_nlp = BertNLP()
    init_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì´ˆê¸°í™” í›„)
    init_memory = get_memory_usage()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "ê²½ì œ ìœ„ê¸°ì™€ ê¸ˆìœµ ì •ì±…ì— ëŒ€í•œ ë¶„ì„",
        "ì •ì¹˜ ê°œí˜ê³¼ ë¯¼ì£¼ì£¼ì˜ ë°œì „",
        "ìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ì™€ ì„ ìˆ˜ë“¤ì˜ í™œì•½",
        "ì‚¬íšŒ ë¬¸ì œì™€ í•´ê²° ë°©ì•ˆ",
        "êµ­ì œ ê´€ê³„ì™€ ì™¸êµ ì •ì±…"
    ] * 20  # 100ê°œ í…ìŠ¤íŠ¸
    
    # ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
    start_time = time.time()
    embeddings = []
    for text in test_texts:
        embedding = bert_nlp.get_bert_embedding(text)
        embeddings.append(embedding)
    embedding_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì²˜ë¦¬ í›„)
    final_memory = get_memory_usage()
    
    # ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
    query = "ê²½ì œì™€ ê¸ˆìœµì— ê´€í•œ ë‚´ìš©"
    start_time = time.time()
    
    similarities = []
    for text in test_texts:
        similarity = bert_nlp.calculate_contextual_similarity(query, text)
        similarities.append(similarity)
    
    similarity_time = time.time() - start_time
    
    # ê²°ê³¼ ì •ë¦¬
    results = {
        'init_time': init_time,
        'embedding_time': embedding_time,
        'similarity_time': similarity_time,
        'total_time': init_time + embedding_time + similarity_time,
        'initial_memory': initial_memory,
        'init_memory': init_memory,
        'final_memory': final_memory,
        'memory_increase': final_memory - initial_memory,
        'texts_processed': len(test_texts),
        'processing_speed': len(test_texts) / embedding_time
    }
    
    logger.info(f"ğŸ“Š CPU BERT ê²°ê³¼:")
    logger.info(f"   - ì´ˆê¸°í™” ì‹œê°„: {init_time:.2f}ì´ˆ")
    logger.info(f"   - ì„ë² ë”© ìƒì„± ì‹œê°„: {embedding_time:.2f}ì´ˆ")
    logger.info(f"   - ìœ ì‚¬ë„ ê³„ì‚° ì‹œê°„: {similarity_time:.2f}ì´ˆ")
    logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {results['total_time']:.2f}ì´ˆ")
    logger.info(f"   - ì²˜ë¦¬ ì†ë„: {results['processing_speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    logger.info(f"   - ë©”ëª¨ë¦¬ ì¦ê°€: {results['memory_increase']:.2f}MB")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del bert_nlp
    gc.collect()
    
    return results

def test_gpu_bert():
    """GPU BERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸš€ GPU BERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not torch.cuda.is_available():
        logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU í…ŒìŠ¤íŠ¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return test_cpu_bert()
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê¸°
    initial_memory = get_memory_usage()
    
    # GPU BERT ì´ˆê¸°í™”
    start_time = time.time()
    bert_nlp = GPUBertNLP()
    init_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì´ˆê¸°í™” í›„)
    init_memory = get_memory_usage()
    
    # GPU í†µê³„ ì¶œë ¥
    gpu_stats = bert_nlp.get_gpu_stats()
    logger.info(f"ğŸ® GPU í†µê³„: {gpu_stats}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "ê²½ì œ ìœ„ê¸°ì™€ ê¸ˆìœµ ì •ì±…ì— ëŒ€í•œ ë¶„ì„",
        "ì •ì¹˜ ê°œí˜ê³¼ ë¯¼ì£¼ì£¼ì˜ ë°œì „",
        "ìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ì™€ ì„ ìˆ˜ë“¤ì˜ í™œì•½",
        "ì‚¬íšŒ ë¬¸ì œì™€ í•´ê²° ë°©ì•ˆ",
        "êµ­ì œ ê´€ê³„ì™€ ì™¸êµ ì •ì±…"
    ] * 20  # 100ê°œ í…ìŠ¤íŠ¸
    
    # GPU ë°°ì¹˜ ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
    start_time = time.time()
    embeddings = bert_nlp.get_embeddings_batch_gpu(test_texts, batch_size=32)
    embedding_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì²˜ë¦¬ í›„)
    final_memory = get_memory_usage()
    
    # GPU ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
    query = "ê²½ì œì™€ ê¸ˆìœµì— ê´€í•œ ë‚´ìš©"
    start_time = time.time()
    
    similarities = bert_nlp.calculate_similarities_batch_gpu(query, test_texts)
    
    similarity_time = time.time() - start_time
    
    # ìºì‹œ ì €ì¥
    bert_nlp.save_cache()
    
    # ê²°ê³¼ ì •ë¦¬
    results = {
        'init_time': init_time,
        'embedding_time': embedding_time,
        'similarity_time': similarity_time,
        'total_time': init_time + embedding_time + similarity_time,
        'initial_memory': initial_memory,
        'init_memory': init_memory,
        'final_memory': final_memory,
        'memory_increase': final_memory - initial_memory,
        'texts_processed': len(test_texts),
        'processing_speed': len(test_texts) / embedding_time,
        'gpu_stats': gpu_stats
    }
    
    logger.info(f"ğŸ“Š GPU BERT ê²°ê³¼:")
    logger.info(f"   - ì´ˆê¸°í™” ì‹œê°„: {init_time:.2f}ì´ˆ")
    logger.info(f"   - ì„ë² ë”© ìƒì„± ì‹œê°„: {embedding_time:.2f}ì´ˆ")
    logger.info(f"   - ìœ ì‚¬ë„ ê³„ì‚° ì‹œê°„: {similarity_time:.2f}ì´ˆ")
    logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {results['total_time']:.2f}ì´ˆ")
    logger.info(f"   - ì²˜ë¦¬ ì†ë„: {results['processing_speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    logger.info(f"   - ë©”ëª¨ë¦¬ ì¦ê°€: {results['memory_increase']:.2f}MB")
    logger.info(f"   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {gpu_stats['memory_usage_percent']:.1f}%")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del bert_nlp
    gc.collect()
    
    return results

def test_batch_size_performance():
    """ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ“¦ ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    if not torch.cuda.is_available():
        logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU í…ŒìŠ¤íŠ¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "ê²½ì œ ìœ„ê¸°ì™€ ê¸ˆìœµ ì •ì±…ì— ëŒ€í•œ ë¶„ì„",
        "ì •ì¹˜ ê°œí˜ê³¼ ë¯¼ì£¼ì£¼ì˜ ë°œì „",
        "ìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ì™€ ì„ ìˆ˜ë“¤ì˜ í™œì•½",
        "ì‚¬íšŒ ë¬¸ì œì™€ í•´ê²° ë°©ì•ˆ",
        "êµ­ì œ ê´€ê³„ì™€ ì™¸êµ ì •ì±…"
    ] * 40  # 200ê°œ í…ìŠ¤íŠ¸
    
    # ë°°ì¹˜ í¬ê¸°ë³„ í…ŒìŠ¤íŠ¸
    batch_sizes = [8, 16, 32, 64, 128]
    
    results = {}
    
    for batch_size in batch_sizes:
        logger.info(f"ğŸ” ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # GPU BERT ì´ˆê¸°í™”
        bert_nlp = GPUBertNLP()
        
        # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        bert_nlp.clear_gpu_cache()
        
        # ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        start_time = time.time()
        embeddings = bert_nlp.get_embeddings_batch_gpu(test_texts, batch_size=batch_size)
        end_time = time.time()
        
        # GPU í†µê³„
        gpu_stats = bert_nlp.get_gpu_stats()
        
        # ê²°ê³¼ ì €ì¥
        results[batch_size] = {
            'time': end_time - start_time,
            'speed': len(test_texts) / (end_time - start_time),
            'memory_usage': gpu_stats['memory_usage_percent']
        }
        
        logger.info(f"   - ì²˜ë¦¬ ì‹œê°„: {results[batch_size]['time']:.2f}ì´ˆ")
        logger.info(f"   - ì²˜ë¦¬ ì†ë„: {results[batch_size]['speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
        logger.info(f"   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {results[batch_size]['memory_usage']:.1f}%")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del bert_nlp
        gc.collect()
    
    # ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    best_batch_size = max(results.keys(), key=lambda x: results[x]['speed'])
    logger.info(f"ğŸ† ìµœì  ë°°ì¹˜ í¬ê¸°: {best_batch_size}")
    logger.info(f"   - ìµœê³  ì²˜ë¦¬ ì†ë„: {results[best_batch_size]['speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    
    return results

def compare_gpu_cpu_performance():
    """GPU vs CPU ì„±ëŠ¥ ë¹„êµ"""
    logger.info("âš–ï¸ GPU vs CPU ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    
    # CPU BERT í…ŒìŠ¤íŠ¸
    cpu_results = test_cpu_bert()
    
    # ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬)
    time.sleep(2)
    gc.collect()
    
    # GPU BERT í…ŒìŠ¤íŠ¸
    gpu_results = test_gpu_bert()
    
    # ì„±ëŠ¥ ë¹„êµ
    logger.info("\n" + "="*60)
    logger.info("ğŸ“ˆ GPU vs CPU ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    logger.info("="*60)
    
    # ì²˜ë¦¬ ì‹œê°„ ë¹„êµ
    time_improvement = (cpu_results['total_time'] - gpu_results['total_time']) / cpu_results['total_time'] * 100
    logger.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„:")
    logger.info(f"   - CPU BERT: {cpu_results['total_time']:.2f}ì´ˆ")
    logger.info(f"   - GPU BERT: {gpu_results['total_time']:.2f}ì´ˆ")
    logger.info(f"   - ê°œì„ ìœ¨: {time_improvement:.1f}%")
    
    # ì²˜ë¦¬ ì†ë„ ë¹„êµ
    speed_improvement = (gpu_results['processing_speed'] - cpu_results['processing_speed']) / cpu_results['processing_speed'] * 100
    logger.info(f"ğŸš€ ì²˜ë¦¬ ì†ë„:")
    logger.info(f"   - CPU BERT: {cpu_results['processing_speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    logger.info(f"   - GPU BERT: {gpu_results['processing_speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    logger.info(f"   - ê°œì„ ìœ¨: {speed_improvement:.1f}%")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
    memory_improvement = (cpu_results['memory_increase'] - gpu_results['memory_increase']) / cpu_results['memory_increase'] * 100
    logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    logger.info(f"   - CPU BERT: {cpu_results['memory_increase']:.2f}MB ì¦ê°€")
    logger.info(f"   - GPU BERT: {gpu_results['memory_increase']:.2f}MB ì¦ê°€")
    logger.info(f"   - ê°œì„ ìœ¨: {memory_improvement:.1f}%")
    
    # ì´ˆê¸°í™” ì‹œê°„ ë¹„êµ
    init_improvement = (cpu_results['init_time'] - gpu_results['init_time']) / cpu_results['init_time'] * 100
    logger.info(f"ğŸ”§ ì´ˆê¸°í™” ì‹œê°„:")
    logger.info(f"   - CPU BERT: {cpu_results['init_time']:.2f}ì´ˆ")
    logger.info(f"   - GPU BERT: {gpu_results['init_time']:.2f}ì´ˆ")
    logger.info(f"   - ê°œì„ ìœ¨: {init_improvement:.1f}%")
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ¯ GPU ìµœì í™” íš¨ê³¼ ìš”ì•½:")
    logger.info(f"   - ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•: {time_improvement:.1f}%")
    logger.info(f"   - ì²˜ë¦¬ ì†ë„ í–¥ìƒ: {speed_improvement:.1f}%")
    logger.info(f"   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {memory_improvement:.1f}%")
    logger.info(f"   - ì´ˆê¸°í™” ì†ë„: {init_improvement:.1f}%")
    logger.info("="*60)

def test_real_world_gpu_scenario():
    """ì‹¤ì œ GPU ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸŒ ì‹¤ì œ GPU ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # í¬ë¡¤ëŸ¬ë¡œ ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        crawler = Crowling()
        news_data = crawler.wordExtraction()
        
        logger.info(f"ğŸ“° í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ë°ì´í„°: {news_data}")
        
        # GPU ìµœì í™”ëœ BERTë¡œ ì‹¤ì œ ì¶”ì²œ í…ŒìŠ¤íŠ¸
        from utils.bert_recommendation_gpu import GPUBertRecommendationSystem
        
        start_time = time.time()
        
        recommender = GPUBertRecommendationSystem(
            batch_size=128,
            max_workers=2
        )
        
        # ìºì‹œ ë¡œë“œ
        recommender.load_cache()
        
        # ì¶”ì²œ ì‹¤í–‰
        recommendations = recommender.recommend_books_by_context_gpu(news_data)
        
        total_time = time.time() - start_time
        
        logger.info(f"â±ï¸ ì‹¤ì œ GPU ì¶”ì²œ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"ğŸ“Š ì¶”ì²œ ê²°ê³¼:")
        
        total_recommendations = 0
        for category, recs in recommendations.items():
            logger.info(f"   - {category}: {len(recs)}ê¶Œ")
            total_recommendations += len(recs)
        
        logger.info(f"   ì´ ì¶”ì²œ ìˆ˜: {total_recommendations}ê¶Œ")
        
        # ìºì‹œ ì €ì¥
        recommender.save_cache()
        recommender.close()
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ GPU ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # GPU vs CPU ì„±ëŠ¥ ë¹„êµ
    compare_gpu_cpu_performance()
    
    # ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    test_batch_size_performance()
    
    # ì‹¤ì œ GPU ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    test_real_world_gpu_scenario()

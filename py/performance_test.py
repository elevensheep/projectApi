#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
BERT ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
- ê¸°ì¡´ BERT vs ìµœì í™”ëœ BERT
- ì²˜ë¦¬ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
"""

import sys
import os
import time
import psutil
import gc

# app í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
app_dir = os.path.join(current_dir, 'app')
sys.path.append(app_dir)

from services.bert_nlp import BertNLP
from services.bert_nlp_optimized import OptimizedBertNLP
from services.crowling import Crowling
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_memory_usage():
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

def test_original_bert():
    """ê¸°ì¡´ BERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª ê¸°ì¡´ BERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê¸°
    initial_memory = get_memory_usage()
    
    # ê¸°ì¡´ BERT ì´ˆê¸°í™”
    start_time = time.time()
    bert_nlp = BertNLP()
    init_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì´ˆê¸°í™” í›„)
    init_memory = get_memory_usage()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "ê²½ì œ ìœ„ê¸°ì™€ ê¸ˆìœµ ì •ì±…ì— ëŒ€í•œ ë¶„ì„",
        "ì •ì¹˜ ê°œí˜ê³¼ ë¯¼ì£¼ì£¼ì˜ ë°œì „",
        "ìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ì™€ ì„ ìˆ˜ë“¤ì˜ í™œì•½",
        "ì‚¬íšŒ ë¬¸ì œì™€ í•´ê²° ë°©ì•ˆ",
        "êµ­ì œ ê´€ê³„ì™€ ì™¸êµ ì •ì±…"
    ] * 10  # 50ê°œ í…ìŠ¤íŠ¸
    
    # ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
    start_time = time.time()
    embeddings = []
    for text in test_texts:
        embedding = bert_nlp.get_bert_embedding(text)
        embeddings.append(embedding)
    embedding_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì²˜ë¦¬ í›„)
    final_memory = get_memory_usage()
    
    # ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
    query = "ê²½ì œì™€ ê¸ˆìœµì— ê´€í•œ ë‚´ìš©"
    start_time = time.time()
    
    similarities = []
    for text in test_texts:
        similarity = bert_nlp.calculate_contextual_similarity(query, text)
        similarities.append(similarity)
    
    similarity_time = time.time() - start_time
    
    # ê²°ê³¼ ì •ë¦¬
    results = {
        'init_time': init_time,
        'embedding_time': embedding_time,
        'similarity_time': similarity_time,
        'total_time': init_time + embedding_time + similarity_time,
        'initial_memory': initial_memory,
        'init_memory': init_memory,
        'final_memory': final_memory,
        'memory_increase': final_memory - initial_memory,
        'texts_processed': len(test_texts),
        'processing_speed': len(test_texts) / embedding_time
    }
    
    logger.info(f"ğŸ“Š ê¸°ì¡´ BERT ê²°ê³¼:")
    logger.info(f"   - ì´ˆê¸°í™” ì‹œê°„: {init_time:.2f}ì´ˆ")
    logger.info(f"   - ì„ë² ë”© ìƒì„± ì‹œê°„: {embedding_time:.2f}ì´ˆ")
    logger.info(f"   - ìœ ì‚¬ë„ ê³„ì‚° ì‹œê°„: {similarity_time:.2f}ì´ˆ")
    logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {results['total_time']:.2f}ì´ˆ")
    logger.info(f"   - ì²˜ë¦¬ ì†ë„: {results['processing_speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    logger.info(f"   - ë©”ëª¨ë¦¬ ì¦ê°€: {results['memory_increase']:.2f}MB")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del bert_nlp
    gc.collect()
    
    return results

def test_optimized_bert():
    """ìµœì í™”ëœ BERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸš€ ìµœì í™”ëœ BERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê¸°
    initial_memory = get_memory_usage()
    
    # ìµœì í™”ëœ BERT ì´ˆê¸°í™”
    start_time = time.time()
    bert_nlp = OptimizedBertNLP()
    init_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì´ˆê¸°í™” í›„)
    init_memory = get_memory_usage()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "ê²½ì œ ìœ„ê¸°ì™€ ê¸ˆìœµ ì •ì±…ì— ëŒ€í•œ ë¶„ì„",
        "ì •ì¹˜ ê°œí˜ê³¼ ë¯¼ì£¼ì£¼ì˜ ë°œì „",
        "ìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ì™€ ì„ ìˆ˜ë“¤ì˜ í™œì•½",
        "ì‚¬íšŒ ë¬¸ì œì™€ í•´ê²° ë°©ì•ˆ",
        "êµ­ì œ ê´€ê³„ì™€ ì™¸êµ ì •ì±…"
    ] * 10  # 50ê°œ í…ìŠ¤íŠ¸
    
    # ë°°ì¹˜ ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
    start_time = time.time()
    embeddings = bert_nlp.get_embeddings_batch(test_texts, batch_size=32)
    embedding_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì²˜ë¦¬ í›„)
    final_memory = get_memory_usage()
    
    # ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
    query = "ê²½ì œì™€ ê¸ˆìœµì— ê´€í•œ ë‚´ìš©"
    start_time = time.time()
    
    similarities = bert_nlp.calculate_similarities_batch(query, test_texts)
    
    similarity_time = time.time() - start_time
    
    # ìºì‹œ ì €ì¥
    bert_nlp.save_cache()
    
    # ê²°ê³¼ ì •ë¦¬
    results = {
        'init_time': init_time,
        'embedding_time': embedding_time,
        'similarity_time': similarity_time,
        'total_time': init_time + embedding_time + similarity_time,
        'initial_memory': initial_memory,
        'init_memory': init_memory,
        'final_memory': final_memory,
        'memory_increase': final_memory - initial_memory,
        'texts_processed': len(test_texts),
        'processing_speed': len(test_texts) / embedding_time,
        'cache_stats': bert_nlp.get_cache_stats()
    }
    
    logger.info(f"ğŸ“Š ìµœì í™”ëœ BERT ê²°ê³¼:")
    logger.info(f"   - ì´ˆê¸°í™” ì‹œê°„: {init_time:.2f}ì´ˆ")
    logger.info(f"   - ì„ë² ë”© ìƒì„± ì‹œê°„: {embedding_time:.2f}ì´ˆ")
    logger.info(f"   - ìœ ì‚¬ë„ ê³„ì‚° ì‹œê°„: {similarity_time:.2f}ì´ˆ")
    logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {results['total_time']:.2f}ì´ˆ")
    logger.info(f"   - ì²˜ë¦¬ ì†ë„: {results['processing_speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    logger.info(f"   - ë©”ëª¨ë¦¬ ì¦ê°€: {results['memory_increase']:.2f}MB")
    logger.info(f"   - ìºì‹œ í†µê³„: {results['cache_stats']}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del bert_nlp
    gc.collect()
    
    return results

def compare_performance():
    """ì„±ëŠ¥ ë¹„êµ"""
    logger.info("âš–ï¸ BERT ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    
    # ê¸°ì¡´ BERT í…ŒìŠ¤íŠ¸
    original_results = test_original_bert()
    
    # ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬)
    time.sleep(2)
    gc.collect()
    
    # ìµœì í™”ëœ BERT í…ŒìŠ¤íŠ¸
    optimized_results = test_optimized_bert()
    
    # ì„±ëŠ¥ ë¹„êµ
    logger.info("\n" + "="*60)
    logger.info("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    logger.info("="*60)
    
    # ì²˜ë¦¬ ì‹œê°„ ë¹„êµ
    time_improvement = (original_results['total_time'] - optimized_results['total_time']) / original_results['total_time'] * 100
    logger.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„:")
    logger.info(f"   - ê¸°ì¡´ BERT: {original_results['total_time']:.2f}ì´ˆ")
    logger.info(f"   - ìµœì í™”ëœ BERT: {optimized_results['total_time']:.2f}ì´ˆ")
    logger.info(f"   - ê°œì„ ìœ¨: {time_improvement:.1f}%")
    
    # ì²˜ë¦¬ ì†ë„ ë¹„êµ
    speed_improvement = (optimized_results['processing_speed'] - original_results['processing_speed']) / original_results['processing_speed'] * 100
    logger.info(f"ğŸš€ ì²˜ë¦¬ ì†ë„:")
    logger.info(f"   - ê¸°ì¡´ BERT: {original_results['processing_speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    logger.info(f"   - ìµœì í™”ëœ BERT: {optimized_results['processing_speed']:.2f} í…ìŠ¤íŠ¸/ì´ˆ")
    logger.info(f"   - ê°œì„ ìœ¨: {speed_improvement:.1f}%")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
    memory_improvement = (original_results['memory_increase'] - optimized_results['memory_increase']) / original_results['memory_increase'] * 100
    logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    logger.info(f"   - ê¸°ì¡´ BERT: {original_results['memory_increase']:.2f}MB ì¦ê°€")
    logger.info(f"   - ìµœì í™”ëœ BERT: {optimized_results['memory_increase']:.2f}MB ì¦ê°€")
    logger.info(f"   - ê°œì„ ìœ¨: {memory_improvement:.1f}%")
    
    # ì´ˆê¸°í™” ì‹œê°„ ë¹„êµ
    init_improvement = (original_results['init_time'] - optimized_results['init_time']) / original_results['init_time'] * 100
    logger.info(f"ğŸ”§ ì´ˆê¸°í™” ì‹œê°„:")
    logger.info(f"   - ê¸°ì¡´ BERT: {original_results['init_time']:.2f}ì´ˆ")
    logger.info(f"   - ìµœì í™”ëœ BERT: {optimized_results['init_time']:.2f}ì´ˆ")
    logger.info(f"   - ê°œì„ ìœ¨: {init_improvement:.1f}%")
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ¯ ìµœì í™” íš¨ê³¼ ìš”ì•½:")
    logger.info(f"   - ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•: {time_improvement:.1f}%")
    logger.info(f"   - ì²˜ë¦¬ ì†ë„ í–¥ìƒ: {speed_improvement:.1f}%")
    logger.info(f"   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {memory_improvement:.1f}%")
    logger.info(f"   - ì´ˆê¸°í™” ì†ë„: {init_improvement:.1f}%")
    logger.info("="*60)

def test_real_world_scenario():
    """ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸŒ ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # í¬ë¡¤ëŸ¬ë¡œ ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        crawler = Crowling()
        news_data = crawler.wordExtraction()
        
        logger.info(f"ğŸ“° í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ë°ì´í„°: {news_data}")
        
        # ìµœì í™”ëœ BERTë¡œ ì‹¤ì œ ì¶”ì²œ í…ŒìŠ¤íŠ¸
        from utils.bert_recommendation_optimized import OptimizedBertRecommendationSystem
        
        start_time = time.time()
        
        recommender = OptimizedBertRecommendationSystem(
            batch_size=64,
            max_workers=4
        )
        
        # ìºì‹œ ë¡œë“œ
        recommender.load_cache()
        
        # ì¶”ì²œ ì‹¤í–‰
        recommendations = recommender.recommend_books_by_context_optimized(news_data)
        
        total_time = time.time() - start_time
        
        logger.info(f"â±ï¸ ì‹¤ì œ ì¶”ì²œ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"ğŸ“Š ì¶”ì²œ ê²°ê³¼:")
        
        total_recommendations = 0
        for category, recs in recommendations.items():
            logger.info(f"   - {category}: {len(recs)}ê¶Œ")
            total_recommendations += len(recs)
        
        logger.info(f"   ì´ ì¶”ì²œ ìˆ˜: {total_recommendations}ê¶Œ")
        
        # ìºì‹œ ì €ì¥
        recommender.save_cache()
        recommender.close()
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
    compare_performance()
    
    # ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    test_real_world_scenario()
